{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89cf913f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66a36635",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cba2e247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3      4     5     6       7     8     9    10  11  12\n",
       "0   7.4  0.70  0.00  1.9  0.076  11.0  34.0  0.9978  3.51  0.56  9.4   5   1\n",
       "1   7.8  0.88  0.00  2.6  0.098  25.0  67.0  0.9968  3.20  0.68  9.8   5   1\n",
       "2   7.8  0.76  0.04  2.3  0.092  15.0  54.0  0.9970  3.26  0.65  9.8   5   1\n",
       "3  11.2  0.28  0.56  1.9  0.075  17.0  60.0  0.9980  3.16  0.58  9.8   6   1\n",
       "4   7.4  0.70  0.00  1.9  0.076  11.0  34.0  0.9978  3.51  0.56  9.4   5   1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./deep_data/wine.csv', header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "845c7a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6497 entries, 0 to 6496\n",
      "Data columns (total 13 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       6497 non-null   float64\n",
      " 1   1       6497 non-null   float64\n",
      " 2   2       6497 non-null   float64\n",
      " 3   3       6497 non-null   float64\n",
      " 4   4       6497 non-null   float64\n",
      " 5   5       6497 non-null   float64\n",
      " 6   6       6497 non-null   float64\n",
      " 7   7       6497 non-null   float64\n",
      " 8   8       6497 non-null   float64\n",
      " 9   9       6497 non-null   float64\n",
      " 10  10      6497 non-null   float64\n",
      " 11  11      6497 non-null   int64  \n",
      " 12  12      6497 non-null   int64  \n",
      "dtypes: float64(11), int64(2)\n",
      "memory usage: 660.0 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95b73ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       1\n",
       "2       1\n",
       "3       1\n",
       "4       1\n",
       "       ..\n",
       "6492    0\n",
       "6493    0\n",
       "6494    0\n",
       "6495    0\n",
       "6496    0\n",
       "Name: 12, Length: 6497, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be5933e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48bb932c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary classification, muliti classification 두 가지 방식으로 nural network model을 만들고 \n",
    "# train data로 학습시킨 후 test data로 accuracy를 평가하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15ea5140",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.iloc[:, 12].values\n",
    "x = df.iloc[:, :12].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ee6a1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_oh = tf.keras.utils.to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027a13e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62a70b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c98cbeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90c9764a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6497, 12)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d13070cf",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 36)                468       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 18)                666       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 9)                 171       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 10        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,315\n",
      "Trainable params: 1,315\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(36, activation='relu', input_dim=12))\n",
    "model.add(Dense(18, activation='relu'))\n",
    "model.add(Dense(9, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2e2fe63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3674f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4872, 12)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d69eca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4872,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3bbd1474",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     0\n",
       "2     0\n",
       "3     0\n",
       "4     0\n",
       "5     0\n",
       "6     0\n",
       "7     0\n",
       "8     0\n",
       "9     0\n",
       "10    0\n",
       "11    0\n",
       "12    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfef521d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "244/244 [==============================] - 2s 5ms/step - loss: 0.2259 - accuracy: 0.9282\n",
      "Epoch 2/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.1755 - accuracy: 0.9436\n",
      "Epoch 3/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.1512 - accuracy: 0.9505\n",
      "Epoch 4/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.1370 - accuracy: 0.9505\n",
      "Epoch 5/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.1168 - accuracy: 0.9604\n",
      "Epoch 6/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.1138 - accuracy: 0.9635\n",
      "Epoch 7/50\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 0.1044 - accuracy: 0.9655\n",
      "Epoch 8/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0966 - accuracy: 0.9688\n",
      "Epoch 9/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0922 - accuracy: 0.9702\n",
      "Epoch 10/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0935 - accuracy: 0.9694\n",
      "Epoch 11/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0887 - accuracy: 0.9737\n",
      "Epoch 12/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0797 - accuracy: 0.9774\n",
      "Epoch 13/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0808 - accuracy: 0.9733\n",
      "Epoch 14/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0772 - accuracy: 0.9760\n",
      "Epoch 15/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0764 - accuracy: 0.9741\n",
      "Epoch 16/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0745 - accuracy: 0.9801\n",
      "Epoch 17/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0721 - accuracy: 0.9787\n",
      "Epoch 18/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0702 - accuracy: 0.9784\n",
      "Epoch 19/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0784 - accuracy: 0.9782\n",
      "Epoch 20/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0691 - accuracy: 0.9762\n",
      "Epoch 21/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0630 - accuracy: 0.9813\n",
      "Epoch 22/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0712 - accuracy: 0.9784\n",
      "Epoch 23/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0700 - accuracy: 0.9774\n",
      "Epoch 24/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0654 - accuracy: 0.9791\n",
      "Epoch 25/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0685 - accuracy: 0.9766\n",
      "Epoch 26/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0645 - accuracy: 0.9803\n",
      "Epoch 27/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0642 - accuracy: 0.9803\n",
      "Epoch 28/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0590 - accuracy: 0.9817\n",
      "Epoch 29/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0659 - accuracy: 0.9795\n",
      "Epoch 30/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0565 - accuracy: 0.9838\n",
      "Epoch 31/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0651 - accuracy: 0.9789\n",
      "Epoch 32/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0595 - accuracy: 0.9801\n",
      "Epoch 33/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0570 - accuracy: 0.9830\n",
      "Epoch 34/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0669 - accuracy: 0.9799\n",
      "Epoch 35/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0644 - accuracy: 0.9791\n",
      "Epoch 36/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0580 - accuracy: 0.9834\n",
      "Epoch 37/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0582 - accuracy: 0.9811\n",
      "Epoch 38/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0566 - accuracy: 0.9832\n",
      "Epoch 39/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0552 - accuracy: 0.9838\n",
      "Epoch 40/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0566 - accuracy: 0.9823\n",
      "Epoch 41/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0570 - accuracy: 0.9830\n",
      "Epoch 42/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0589 - accuracy: 0.9807\n",
      "Epoch 43/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0597 - accuracy: 0.9791\n",
      "Epoch 44/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0582 - accuracy: 0.9819\n",
      "Epoch 45/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0569 - accuracy: 0.9828\n",
      "Epoch 46/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0553 - accuracy: 0.9821\n",
      "Epoch 47/50\n",
      "244/244 [==============================] - 1s 3ms/step - loss: 0.0517 - accuracy: 0.9836\n",
      "Epoch 48/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0563 - accuracy: 0.9832\n",
      "Epoch 49/50\n",
      "244/244 [==============================] - 1s 3ms/step - loss: 0.0521 - accuracy: 0.9844\n",
      "Epoch 50/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0533 - accuracy: 0.9834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23f1b450040>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train.astype(float), y_train, epochs=50, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b959b8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 0s 4ms/step - loss: 0.0605 - accuracy: 0.9822\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.060474853962659836, 0.9821538329124451]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d114d33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128ef595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiclassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d43c8edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b405f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 36)                468       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 18)                666       \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 9)                 171       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 2)                 20        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,325\n",
      "Trainable params: 1,325\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(36, activation='relu', input_dim=12))\n",
    "model.add(Dense(18, activation='relu'))\n",
    "model.add(Dense(9, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71640536",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6504d6f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 0.2856 - accuracy: 0.8957\n",
      "Epoch 2/50\n",
      "244/244 [==============================] - 1s 3ms/step - loss: 0.1788 - accuracy: 0.9339\n",
      "Epoch 3/50\n",
      "244/244 [==============================] - 1s 3ms/step - loss: 0.1576 - accuracy: 0.9423\n",
      "Epoch 4/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.1411 - accuracy: 0.9477\n",
      "Epoch 5/50\n",
      "244/244 [==============================] - 1s 3ms/step - loss: 0.1245 - accuracy: 0.9540\n",
      "Epoch 6/50\n",
      "244/244 [==============================] - 1s 3ms/step - loss: 0.1202 - accuracy: 0.9600\n",
      "Epoch 7/50\n",
      "244/244 [==============================] - 1s 3ms/step - loss: 0.1029 - accuracy: 0.9659\n",
      "Epoch 8/50\n",
      "244/244 [==============================] - 1s 3ms/step - loss: 0.1013 - accuracy: 0.9655\n",
      "Epoch 9/50\n",
      "244/244 [==============================] - 1s 3ms/step - loss: 0.0932 - accuracy: 0.9700\n",
      "Epoch 10/50\n",
      "244/244 [==============================] - 1s 3ms/step - loss: 0.0942 - accuracy: 0.9688\n",
      "Epoch 11/50\n",
      "244/244 [==============================] - 1s 3ms/step - loss: 0.0901 - accuracy: 0.9709\n",
      "Epoch 12/50\n",
      "244/244 [==============================] - 1s 3ms/step - loss: 0.0824 - accuracy: 0.9750\n",
      "Epoch 13/50\n",
      "244/244 [==============================] - 1s 3ms/step - loss: 0.0775 - accuracy: 0.9741\n",
      "Epoch 14/50\n",
      "244/244 [==============================] - 1s 3ms/step - loss: 0.0765 - accuracy: 0.9743\n",
      "Epoch 15/50\n",
      "244/244 [==============================] - 1s 3ms/step - loss: 0.0757 - accuracy: 0.9764\n",
      "Epoch 16/50\n",
      "244/244 [==============================] - 1s 3ms/step - loss: 0.0714 - accuracy: 0.9774\n",
      "Epoch 17/50\n",
      "244/244 [==============================] - 1s 3ms/step - loss: 0.0771 - accuracy: 0.9762\n",
      "Epoch 18/50\n",
      "244/244 [==============================] - 1s 3ms/step - loss: 0.0720 - accuracy: 0.9762\n",
      "Epoch 19/50\n",
      "244/244 [==============================] - 1s 3ms/step - loss: 0.0766 - accuracy: 0.9750\n",
      "Epoch 20/50\n",
      "244/244 [==============================] - 1s 3ms/step - loss: 0.0719 - accuracy: 0.9772\n",
      "Epoch 21/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0622 - accuracy: 0.9795\n",
      "Epoch 22/50\n",
      "244/244 [==============================] - 1s 3ms/step - loss: 0.0693 - accuracy: 0.9793\n",
      "Epoch 23/50\n",
      "244/244 [==============================] - 1s 3ms/step - loss: 0.0741 - accuracy: 0.9774\n",
      "Epoch 24/50\n",
      "244/244 [==============================] - 1s 3ms/step - loss: 0.0654 - accuracy: 0.9770\n",
      "Epoch 25/50\n",
      "244/244 [==============================] - 1s 3ms/step - loss: 0.0704 - accuracy: 0.9766\n",
      "Epoch 26/50\n",
      "244/244 [==============================] - 1s 3ms/step - loss: 0.0699 - accuracy: 0.9774\n",
      "Epoch 27/50\n",
      "244/244 [==============================] - 1s 3ms/step - loss: 0.0715 - accuracy: 0.9766\n",
      "Epoch 28/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0660 - accuracy: 0.9799\n",
      "Epoch 29/50\n",
      "244/244 [==============================] - 1s 3ms/step - loss: 0.0608 - accuracy: 0.9793\n",
      "Epoch 30/50\n",
      "244/244 [==============================] - 1s 3ms/step - loss: 0.0610 - accuracy: 0.9799\n",
      "Epoch 31/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0790 - accuracy: 0.9770\n",
      "Epoch 32/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0610 - accuracy: 0.9801\n",
      "Epoch 33/50\n",
      "244/244 [==============================] - 1s 3ms/step - loss: 0.0697 - accuracy: 0.9780\n",
      "Epoch 34/50\n",
      "244/244 [==============================] - 1s 3ms/step - loss: 0.0766 - accuracy: 0.9768\n",
      "Epoch 35/50\n",
      "244/244 [==============================] - 1s 3ms/step - loss: 0.0643 - accuracy: 0.9793\n",
      "Epoch 36/50\n",
      "244/244 [==============================] - 1s 3ms/step - loss: 0.0622 - accuracy: 0.9793\n",
      "Epoch 37/50\n",
      "244/244 [==============================] - 1s 3ms/step - loss: 0.0635 - accuracy: 0.9803\n",
      "Epoch 38/50\n",
      "244/244 [==============================] - 1s 3ms/step - loss: 0.0660 - accuracy: 0.9782\n",
      "Epoch 39/50\n",
      "244/244 [==============================] - 1s 3ms/step - loss: 0.0571 - accuracy: 0.9813\n",
      "Epoch 40/50\n",
      "244/244 [==============================] - 1s 3ms/step - loss: 0.0568 - accuracy: 0.9813\n",
      "Epoch 41/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0611 - accuracy: 0.9805\n",
      "Epoch 42/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0578 - accuracy: 0.9823\n",
      "Epoch 43/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0582 - accuracy: 0.9813\n",
      "Epoch 44/50\n",
      "244/244 [==============================] - 1s 4ms/step - loss: 0.0546 - accuracy: 0.9828\n",
      "Epoch 45/50\n",
      "244/244 [==============================] - 1s 3ms/step - loss: 0.0538 - accuracy: 0.9840\n",
      "Epoch 46/50\n",
      "244/244 [==============================] - 1s 3ms/step - loss: 0.0578 - accuracy: 0.9813\n",
      "Epoch 47/50\n",
      "244/244 [==============================] - 1s 3ms/step - loss: 0.0593 - accuracy: 0.9801\n",
      "Epoch 48/50\n",
      "244/244 [==============================] - 1s 3ms/step - loss: 0.0523 - accuracy: 0.9844\n",
      "Epoch 49/50\n",
      "244/244 [==============================] - 1s 3ms/step - loss: 0.0556 - accuracy: 0.9830\n",
      "Epoch 50/50\n",
      "244/244 [==============================] - 1s 3ms/step - loss: 0.0664 - accuracy: 0.9784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23f30695f40>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train.astype(float), y_train, epochs=50, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d7abb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 0s 4ms/step - loss: 0.0582 - accuracy: 0.9797\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0582406111061573, 0.979692280292511]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70d27b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./deep_model/wine_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd04ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6299d3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model checkpointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a0f617c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y_oh)\n",
    "model = Sequential()\n",
    "model.add(Dense(36, activation='relu', input_dim=12))\n",
    "model.add(Dense(18, activation='relu'))\n",
    "model.add(Dense(9, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "# model.summary()\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "251dff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a4ae78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = './deep_model/model_check'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6fa0dbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62d48a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = './deep_model/model_check/{epoch:02d}-{val_loss:4f}.hdf5'\n",
    "checkpointer = ModelCheckpoint(filepath = modelpath, monitor='val_loss', verbose=1, \\\n",
    "                               save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2124e008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.5308 - accuracy: 0.8759\n",
      "Epoch 1: val_loss improved from inf to 0.22453, saving model to ./deep_model/model_check\\01-0.224531.hdf5\n",
      "78/78 [==============================] - 1s 7ms/step - loss: 0.5100 - accuracy: 0.8799 - val_loss: 0.2245 - val_accuracy: 0.9303\n",
      "Epoch 2/100\n",
      "72/78 [==========================>...] - ETA: 0s - loss: 0.2003 - accuracy: 0.9286\n",
      "Epoch 2: val_loss improved from 0.22453 to 0.18799, saving model to ./deep_model/model_check\\02-0.187988.hdf5\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.1983 - accuracy: 0.9299 - val_loss: 0.1880 - val_accuracy: 0.9364\n",
      "Epoch 3/100\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.1818 - accuracy: 0.9337\n",
      "Epoch 3: val_loss improved from 0.18799 to 0.18340, saving model to ./deep_model/model_check\\03-0.183402.hdf5\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.1831 - accuracy: 0.9323 - val_loss: 0.1834 - val_accuracy: 0.9374\n",
      "Epoch 4/100\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.1740 - accuracy: 0.9349\n",
      "Epoch 4: val_loss improved from 0.18340 to 0.17271, saving model to ./deep_model/model_check\\04-0.172711.hdf5\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.1744 - accuracy: 0.9356 - val_loss: 0.1727 - val_accuracy: 0.9395\n",
      "Epoch 5/100\n",
      "64/78 [=======================>......] - ETA: 0s - loss: 0.1725 - accuracy: 0.9388\n",
      "Epoch 5: val_loss improved from 0.17271 to 0.16899, saving model to ./deep_model/model_check\\05-0.168992.hdf5\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.1724 - accuracy: 0.9400 - val_loss: 0.1690 - val_accuracy: 0.9405\n",
      "Epoch 6/100\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.1615 - accuracy: 0.9442\n",
      "Epoch 6: val_loss improved from 0.16899 to 0.14963, saving model to ./deep_model/model_check\\06-0.149632.hdf5\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.1600 - accuracy: 0.9448 - val_loss: 0.1496 - val_accuracy: 0.9436\n",
      "Epoch 7/100\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.1466 - accuracy: 0.9519\n",
      "Epoch 7: val_loss improved from 0.14963 to 0.14100, saving model to ./deep_model/model_check\\07-0.140999.hdf5\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.1500 - accuracy: 0.9497 - val_loss: 0.1410 - val_accuracy: 0.9456\n",
      "Epoch 8/100\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.1381 - accuracy: 0.9530\n",
      "Epoch 8: val_loss improved from 0.14100 to 0.13418, saving model to ./deep_model/model_check\\08-0.134180.hdf5\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.1371 - accuracy: 0.9518 - val_loss: 0.1342 - val_accuracy: 0.9467\n",
      "Epoch 9/100\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.1309 - accuracy: 0.9555\n",
      "Epoch 9: val_loss did not improve from 0.13418\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.1308 - accuracy: 0.9554 - val_loss: 0.1364 - val_accuracy: 0.9436\n",
      "Epoch 10/100\n",
      "64/78 [=======================>......] - ETA: 0s - loss: 0.1258 - accuracy: 0.9566\n",
      "Epoch 10: val_loss improved from 0.13418 to 0.11584, saving model to ./deep_model/model_check\\10-0.115841.hdf5\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.1215 - accuracy: 0.9582 - val_loss: 0.1158 - val_accuracy: 0.9497\n",
      "Epoch 11/100\n",
      "63/78 [=======================>......] - ETA: 0s - loss: 0.1156 - accuracy: 0.9597\n",
      "Epoch 11: val_loss did not improve from 0.11584\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.1170 - accuracy: 0.9579 - val_loss: 0.1299 - val_accuracy: 0.9456\n",
      "Epoch 12/100\n",
      "63/78 [=======================>......] - ETA: 0s - loss: 0.1060 - accuracy: 0.9644\n",
      "Epoch 12: val_loss improved from 0.11584 to 0.10408, saving model to ./deep_model/model_check\\12-0.104084.hdf5\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.1101 - accuracy: 0.9625 - val_loss: 0.1041 - val_accuracy: 0.9549\n",
      "Epoch 13/100\n",
      "63/78 [=======================>......] - ETA: 0s - loss: 0.1051 - accuracy: 0.9632\n",
      "Epoch 13: val_loss did not improve from 0.10408\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.1048 - accuracy: 0.9646 - val_loss: 0.1142 - val_accuracy: 0.9508\n",
      "Epoch 14/100\n",
      "74/78 [===========================>..] - ETA: 0s - loss: 0.1002 - accuracy: 0.9643\n",
      "Epoch 14: val_loss did not improve from 0.10408\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.1003 - accuracy: 0.9643 - val_loss: 0.1093 - val_accuracy: 0.9538\n",
      "Epoch 15/100\n",
      "74/78 [===========================>..] - ETA: 0s - loss: 0.0939 - accuracy: 0.9678\n",
      "Epoch 15: val_loss improved from 0.10408 to 0.09656, saving model to ./deep_model/model_check\\15-0.096563.hdf5\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0979 - accuracy: 0.9666 - val_loss: 0.0966 - val_accuracy: 0.9641\n",
      "Epoch 16/100\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.1153 - accuracy: 0.9624\n",
      "Epoch 16: val_loss did not improve from 0.09656\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.1100 - accuracy: 0.9641 - val_loss: 0.1490 - val_accuracy: 0.9456\n",
      "Epoch 17/100\n",
      "64/78 [=======================>......] - ETA: 0s - loss: 0.0956 - accuracy: 0.9697\n",
      "Epoch 17: val_loss did not improve from 0.09656\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0921 - accuracy: 0.9707 - val_loss: 0.1131 - val_accuracy: 0.9549\n",
      "Epoch 18/100\n",
      "63/78 [=======================>......] - ETA: 0s - loss: 0.0818 - accuracy: 0.9721\n",
      "Epoch 18: val_loss improved from 0.09656 to 0.08545, saving model to ./deep_model/model_check\\18-0.085450.hdf5\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0797 - accuracy: 0.9741 - val_loss: 0.0854 - val_accuracy: 0.9672\n",
      "Epoch 19/100\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0894 - accuracy: 0.9689\n",
      "Epoch 19: val_loss improved from 0.08545 to 0.08389, saving model to ./deep_model/model_check\\19-0.083894.hdf5\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0882 - accuracy: 0.9692 - val_loss: 0.0839 - val_accuracy: 0.9682\n",
      "Epoch 20/100\n",
      "65/78 [========================>.....] - ETA: 0s - loss: 0.0926 - accuracy: 0.9702\n",
      "Epoch 20: val_loss did not improve from 0.08389\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0926 - accuracy: 0.9697 - val_loss: 0.1036 - val_accuracy: 0.9641\n",
      "Epoch 21/100\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.0793 - accuracy: 0.9745\n",
      "Epoch 21: val_loss improved from 0.08389 to 0.07856, saving model to ./deep_model/model_check\\21-0.078558.hdf5\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.0817 - accuracy: 0.9743 - val_loss: 0.0786 - val_accuracy: 0.9723\n",
      "Epoch 22/100\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0738 - accuracy: 0.9757\n",
      "Epoch 22: val_loss improved from 0.07856 to 0.07457, saving model to ./deep_model/model_check\\22-0.074572.hdf5\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0739 - accuracy: 0.9756 - val_loss: 0.0746 - val_accuracy: 0.9733\n",
      "Epoch 23/100\n",
      "62/78 [======================>.......] - ETA: 0s - loss: 0.0674 - accuracy: 0.9777\n",
      "Epoch 23: val_loss did not improve from 0.07457\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0751 - accuracy: 0.9769 - val_loss: 0.1251 - val_accuracy: 0.9569\n",
      "Epoch 24/100\n",
      "65/78 [========================>.....] - ETA: 0s - loss: 0.0745 - accuracy: 0.9766\n",
      "Epoch 24: val_loss did not improve from 0.07457\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0746 - accuracy: 0.9774 - val_loss: 0.0751 - val_accuracy: 0.9713\n",
      "Epoch 25/100\n",
      "65/78 [========================>.....] - ETA: 0s - loss: 0.0690 - accuracy: 0.9794\n",
      "Epoch 25: val_loss improved from 0.07457 to 0.06943, saving model to ./deep_model/model_check\\25-0.069434.hdf5\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0702 - accuracy: 0.9777 - val_loss: 0.0694 - val_accuracy: 0.9764\n",
      "Epoch 26/100\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0752 - accuracy: 0.9746\n",
      "Epoch 26: val_loss did not improve from 0.06943\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0752 - accuracy: 0.9746 - val_loss: 0.1015 - val_accuracy: 0.9651\n",
      "Epoch 27/100\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0725 - accuracy: 0.9777\n",
      "Epoch 27: val_loss did not improve from 0.06943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0731 - accuracy: 0.9779 - val_loss: 0.0706 - val_accuracy: 0.9733\n",
      "Epoch 28/100\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0635 - accuracy: 0.9794\n",
      "Epoch 28: val_loss did not improve from 0.06943\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.0640 - accuracy: 0.9792 - val_loss: 0.0714 - val_accuracy: 0.9723\n",
      "Epoch 29/100\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0740 - accuracy: 0.9751\n",
      "Epoch 29: val_loss did not improve from 0.06943\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.0727 - accuracy: 0.9751 - val_loss: 0.1012 - val_accuracy: 0.9641\n",
      "Epoch 30/100\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0678 - accuracy: 0.9810\n",
      "Epoch 30: val_loss did not improve from 0.06943\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0678 - accuracy: 0.9810 - val_loss: 0.1282 - val_accuracy: 0.9600\n",
      "Epoch 31/100\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0628 - accuracy: 0.9789\n",
      "Epoch 31: val_loss improved from 0.06943 to 0.06348, saving model to ./deep_model/model_check\\31-0.063483.hdf5\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.0652 - accuracy: 0.9782 - val_loss: 0.0635 - val_accuracy: 0.9774\n",
      "Epoch 32/100\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.0710 - accuracy: 0.9784\n",
      "Epoch 32: val_loss did not improve from 0.06348\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0704 - accuracy: 0.9782 - val_loss: 0.1008 - val_accuracy: 0.9672\n",
      "Epoch 33/100\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.0600 - accuracy: 0.9821\n",
      "Epoch 33: val_loss did not improve from 0.06348\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0640 - accuracy: 0.9810 - val_loss: 0.0649 - val_accuracy: 0.9754\n",
      "Epoch 34/100\n",
      "64/78 [=======================>......] - ETA: 0s - loss: 0.0736 - accuracy: 0.9753\n",
      "Epoch 34: val_loss did not improve from 0.06348\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0722 - accuracy: 0.9759 - val_loss: 0.0703 - val_accuracy: 0.9774\n",
      "Epoch 35/100\n",
      "63/78 [=======================>......] - ETA: 0s - loss: 0.0546 - accuracy: 0.9816\n",
      "Epoch 35: val_loss did not improve from 0.06348\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0579 - accuracy: 0.9810 - val_loss: 0.1154 - val_accuracy: 0.9621\n",
      "Epoch 36/100\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0629 - accuracy: 0.9803\n",
      "Epoch 36: val_loss improved from 0.06348 to 0.06151, saving model to ./deep_model/model_check\\36-0.061511.hdf5\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0630 - accuracy: 0.9800 - val_loss: 0.0615 - val_accuracy: 0.9785\n",
      "Epoch 37/100\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.0787 - accuracy: 0.9731\n",
      "Epoch 37: val_loss did not improve from 0.06151\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0777 - accuracy: 0.9733 - val_loss: 0.0787 - val_accuracy: 0.9733\n",
      "Epoch 38/100\n",
      "74/78 [===========================>..] - ETA: 0s - loss: 0.0567 - accuracy: 0.9824\n",
      "Epoch 38: val_loss improved from 0.06151 to 0.05833, saving model to ./deep_model/model_check\\38-0.058333.hdf5\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0604 - accuracy: 0.9820 - val_loss: 0.0583 - val_accuracy: 0.9795\n",
      "Epoch 39/100\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0749 - accuracy: 0.9766\n",
      "Epoch 39: val_loss did not improve from 0.05833\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0736 - accuracy: 0.9777 - val_loss: 0.0587 - val_accuracy: 0.9805\n",
      "Epoch 40/100\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.0618 - accuracy: 0.9792\n",
      "Epoch 40: val_loss improved from 0.05833 to 0.05747, saving model to ./deep_model/model_check\\40-0.057470.hdf5\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.0624 - accuracy: 0.9784 - val_loss: 0.0575 - val_accuracy: 0.9774\n",
      "Epoch 41/100\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0544 - accuracy: 0.9827\n",
      "Epoch 41: val_loss did not improve from 0.05747\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0551 - accuracy: 0.9838 - val_loss: 0.0687 - val_accuracy: 0.9764\n",
      "Epoch 42/100\n",
      "65/78 [========================>.....] - ETA: 0s - loss: 0.0576 - accuracy: 0.9815\n",
      "Epoch 42: val_loss improved from 0.05747 to 0.05729, saving model to ./deep_model/model_check\\42-0.057290.hdf5\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0575 - accuracy: 0.9815 - val_loss: 0.0573 - val_accuracy: 0.9805\n",
      "Epoch 43/100\n",
      "65/78 [========================>.....] - ETA: 0s - loss: 0.0533 - accuracy: 0.9837\n",
      "Epoch 43: val_loss did not improve from 0.05729\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0559 - accuracy: 0.9828 - val_loss: 0.0697 - val_accuracy: 0.9764\n",
      "Epoch 44/100\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.0707 - accuracy: 0.9791\n",
      "Epoch 44: val_loss did not improve from 0.05729\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0674 - accuracy: 0.9800 - val_loss: 0.0590 - val_accuracy: 0.9805\n",
      "Epoch 45/100\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0592 - accuracy: 0.9827\n",
      "Epoch 45: val_loss did not improve from 0.05729\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0602 - accuracy: 0.9813 - val_loss: 0.0717 - val_accuracy: 0.9723\n",
      "Epoch 46/100\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0711 - accuracy: 0.9773\n",
      "Epoch 46: val_loss improved from 0.05729 to 0.05624, saving model to ./deep_model/model_check\\46-0.056239.hdf5\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0700 - accuracy: 0.9769 - val_loss: 0.0562 - val_accuracy: 0.9795\n",
      "Epoch 47/100\n",
      "72/78 [==========================>...] - ETA: 0s - loss: 0.0535 - accuracy: 0.9833\n",
      "Epoch 47: val_loss did not improve from 0.05624\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0545 - accuracy: 0.9831 - val_loss: 0.0635 - val_accuracy: 0.9785\n",
      "Epoch 48/100\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0545 - accuracy: 0.9847\n",
      "Epoch 48: val_loss did not improve from 0.05624\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0540 - accuracy: 0.9849 - val_loss: 0.0580 - val_accuracy: 0.9795\n",
      "Epoch 49/100\n",
      "72/78 [==========================>...] - ETA: 0s - loss: 0.0653 - accuracy: 0.9781\n",
      "Epoch 49: val_loss did not improve from 0.05624\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0688 - accuracy: 0.9764 - val_loss: 0.0761 - val_accuracy: 0.9744\n",
      "Epoch 50/100\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.0686 - accuracy: 0.9786\n",
      "Epoch 50: val_loss did not improve from 0.05624\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0679 - accuracy: 0.9790 - val_loss: 0.0739 - val_accuracy: 0.9733\n",
      "Epoch 51/100\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0541 - accuracy: 0.9837\n",
      "Epoch 51: val_loss improved from 0.05624 to 0.05429, saving model to ./deep_model/model_check\\51-0.054292.hdf5\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.0533 - accuracy: 0.9836 - val_loss: 0.0543 - val_accuracy: 0.9795\n",
      "Epoch 52/100\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.0524 - accuracy: 0.9845\n",
      "Epoch 52: val_loss did not improve from 0.05429\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.0523 - accuracy: 0.9843 - val_loss: 0.0929 - val_accuracy: 0.9682\n",
      "Epoch 53/100\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0592 - accuracy: 0.9825\n",
      "Epoch 53: val_loss did not improve from 0.05429\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0589 - accuracy: 0.9828 - val_loss: 0.0593 - val_accuracy: 0.9815\n",
      "Epoch 54/100\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.0560 - accuracy: 0.9844\n",
      "Epoch 54: val_loss did not improve from 0.05429\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0567 - accuracy: 0.9843 - val_loss: 0.0704 - val_accuracy: 0.9764\n",
      "Epoch 55/100\n",
      "74/78 [===========================>..] - ETA: 0s - loss: 0.0625 - accuracy: 0.9811\n",
      "Epoch 55: val_loss did not improve from 0.05429\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0633 - accuracy: 0.9800 - val_loss: 0.1127 - val_accuracy: 0.9610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100\n",
      "78/78 [==============================] - ETA: 0s - loss: 0.0724 - accuracy: 0.9774\n",
      "Epoch 56: val_loss did not improve from 0.05429\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0724 - accuracy: 0.9774 - val_loss: 0.0609 - val_accuracy: 0.9774\n",
      "Epoch 57/100\n",
      "74/78 [===========================>..] - ETA: 0s - loss: 0.0523 - accuracy: 0.9846\n",
      "Epoch 57: val_loss did not improve from 0.05429\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0520 - accuracy: 0.9846 - val_loss: 0.0786 - val_accuracy: 0.9733\n",
      "Epoch 58/100\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0631 - accuracy: 0.9789\n",
      "Epoch 58: val_loss did not improve from 0.05429\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0627 - accuracy: 0.9795 - val_loss: 0.0549 - val_accuracy: 0.9785\n",
      "Epoch 59/100\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.0565 - accuracy: 0.9827\n",
      "Epoch 59: val_loss did not improve from 0.05429\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0559 - accuracy: 0.9828 - val_loss: 0.0831 - val_accuracy: 0.9682\n",
      "Epoch 60/100\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.0535 - accuracy: 0.9845\n",
      "Epoch 60: val_loss did not improve from 0.05429\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0517 - accuracy: 0.9846 - val_loss: 0.0664 - val_accuracy: 0.9774\n",
      "Epoch 61/100\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0497 - accuracy: 0.9848\n",
      "Epoch 61: val_loss did not improve from 0.05429\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0522 - accuracy: 0.9843 - val_loss: 0.0655 - val_accuracy: 0.9805\n",
      "Epoch 62/100\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0512 - accuracy: 0.9863\n",
      "Epoch 62: val_loss did not improve from 0.05429\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0527 - accuracy: 0.9854 - val_loss: 0.0900 - val_accuracy: 0.9713\n",
      "Epoch 63/100\n",
      "65/78 [========================>.....] - ETA: 0s - loss: 0.0651 - accuracy: 0.9809\n",
      "Epoch 63: val_loss did not improve from 0.05429\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0627 - accuracy: 0.9813 - val_loss: 0.0594 - val_accuracy: 0.9815\n",
      "Epoch 64/100\n",
      "65/78 [========================>.....] - ETA: 0s - loss: 0.0614 - accuracy: 0.9828\n",
      "Epoch 64: val_loss did not improve from 0.05429\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0568 - accuracy: 0.9843 - val_loss: 0.0571 - val_accuracy: 0.9754\n",
      "Epoch 65/100\n",
      "72/78 [==========================>...] - ETA: 0s - loss: 0.0529 - accuracy: 0.9844\n",
      "Epoch 65: val_loss did not improve from 0.05429\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0555 - accuracy: 0.9841 - val_loss: 0.0801 - val_accuracy: 0.9733\n",
      "Epoch 66/100\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0585 - accuracy: 0.9831\n",
      "Epoch 66: val_loss did not improve from 0.05429\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.0573 - accuracy: 0.9836 - val_loss: 0.0556 - val_accuracy: 0.9795\n",
      "Epoch 67/100\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0547 - accuracy: 0.9864\n",
      "Epoch 67: val_loss did not improve from 0.05429\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.0549 - accuracy: 0.9861 - val_loss: 0.0821 - val_accuracy: 0.9733\n",
      "Epoch 68/100\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0492 - accuracy: 0.9854\n",
      "Epoch 68: val_loss improved from 0.05429 to 0.05174, saving model to ./deep_model/model_check\\68-0.051741.hdf5\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.0512 - accuracy: 0.9851 - val_loss: 0.0517 - val_accuracy: 0.9836\n",
      "Epoch 69/100\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0523 - accuracy: 0.9849\n",
      "Epoch 69: val_loss did not improve from 0.05174\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0503 - accuracy: 0.9854 - val_loss: 0.0534 - val_accuracy: 0.9826\n",
      "Epoch 70/100\n",
      "74/78 [===========================>..] - ETA: 0s - loss: 0.0521 - accuracy: 0.9841\n",
      "Epoch 70: val_loss did not improve from 0.05174\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0516 - accuracy: 0.9843 - val_loss: 0.0552 - val_accuracy: 0.9846\n",
      "Epoch 71/100\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0477 - accuracy: 0.9868\n",
      "Epoch 71: val_loss did not improve from 0.05174\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0469 - accuracy: 0.9872 - val_loss: 0.0597 - val_accuracy: 0.9785\n",
      "Epoch 72/100\n",
      "74/78 [===========================>..] - ETA: 0s - loss: 0.0501 - accuracy: 0.9857\n",
      "Epoch 72: val_loss improved from 0.05174 to 0.05146, saving model to ./deep_model/model_check\\72-0.051458.hdf5\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.0509 - accuracy: 0.9851 - val_loss: 0.0515 - val_accuracy: 0.9815\n",
      "Epoch 73/100\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0440 - accuracy: 0.9869\n",
      "Epoch 73: val_loss did not improve from 0.05146\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0452 - accuracy: 0.9869 - val_loss: 0.0517 - val_accuracy: 0.9826\n",
      "Epoch 74/100\n",
      "72/78 [==========================>...] - ETA: 0s - loss: 0.0435 - accuracy: 0.9875\n",
      "Epoch 74: val_loss did not improve from 0.05146\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0446 - accuracy: 0.9877 - val_loss: 0.0560 - val_accuracy: 0.9795\n",
      "Epoch 75/100\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0471 - accuracy: 0.9858\n",
      "Epoch 75: val_loss did not improve from 0.05146\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.0487 - accuracy: 0.9854 - val_loss: 0.0697 - val_accuracy: 0.9754\n",
      "Epoch 76/100\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0681 - accuracy: 0.9786\n",
      "Epoch 76: val_loss did not improve from 0.05146\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0648 - accuracy: 0.9792 - val_loss: 0.0558 - val_accuracy: 0.9805\n",
      "Epoch 77/100\n",
      "74/78 [===========================>..] - ETA: 0s - loss: 0.0481 - accuracy: 0.9876\n",
      "Epoch 77: val_loss did not improve from 0.05146\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0516 - accuracy: 0.9867 - val_loss: 0.0559 - val_accuracy: 0.9826\n",
      "Epoch 78/100\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0469 - accuracy: 0.9861\n",
      "Epoch 78: val_loss did not improve from 0.05146\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0477 - accuracy: 0.9859 - val_loss: 0.0705 - val_accuracy: 0.9754\n",
      "Epoch 79/100\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.0562 - accuracy: 0.9845\n",
      "Epoch 79: val_loss did not improve from 0.05146\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0513 - accuracy: 0.9854 - val_loss: 0.0532 - val_accuracy: 0.9826\n",
      "Epoch 80/100\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.0501 - accuracy: 0.9861\n",
      "Epoch 80: val_loss did not improve from 0.05146\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0509 - accuracy: 0.9861 - val_loss: 0.0550 - val_accuracy: 0.9805\n",
      "Epoch 81/100\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0449 - accuracy: 0.9877\n",
      "Epoch 81: val_loss did not improve from 0.05146\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0459 - accuracy: 0.9877 - val_loss: 0.1115 - val_accuracy: 0.9672\n",
      "Epoch 82/100\n",
      "74/78 [===========================>..] - ETA: 0s - loss: 0.0510 - accuracy: 0.9859\n",
      "Epoch 82: val_loss did not improve from 0.05146\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0497 - accuracy: 0.9861 - val_loss: 0.0534 - val_accuracy: 0.9795\n",
      "Epoch 83/100\n",
      "63/78 [=======================>......] - ETA: 0s - loss: 0.0551 - accuracy: 0.9841\n",
      "Epoch 83: val_loss did not improve from 0.05146\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0527 - accuracy: 0.9849 - val_loss: 0.0663 - val_accuracy: 0.9785\n",
      "Epoch 84/100\n",
      "72/78 [==========================>...] - ETA: 0s - loss: 0.0486 - accuracy: 0.9864\n",
      "Epoch 84: val_loss did not improve from 0.05146\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0493 - accuracy: 0.9864 - val_loss: 0.0560 - val_accuracy: 0.9815\n",
      "Epoch 85/100\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0666 - accuracy: 0.9803\n",
      "Epoch 85: val_loss did not improve from 0.05146\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0644 - accuracy: 0.9810 - val_loss: 0.0808 - val_accuracy: 0.9703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/100\n",
      "65/78 [========================>.....] - ETA: 0s - loss: 0.0545 - accuracy: 0.9840\n",
      "Epoch 86: val_loss did not improve from 0.05146\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0509 - accuracy: 0.9851 - val_loss: 0.0656 - val_accuracy: 0.9785\n",
      "Epoch 87/100\n",
      "72/78 [==========================>...] - ETA: 0s - loss: 0.0501 - accuracy: 0.9881\n",
      "Epoch 87: val_loss did not improve from 0.05146\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0511 - accuracy: 0.9872 - val_loss: 0.0637 - val_accuracy: 0.9795\n",
      "Epoch 88/100\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.0605 - accuracy: 0.9833\n",
      "Epoch 88: val_loss did not improve from 0.05146\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0590 - accuracy: 0.9838 - val_loss: 0.0525 - val_accuracy: 0.9836\n",
      "Epoch 89/100\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.0486 - accuracy: 0.9855\n",
      "Epoch 89: val_loss did not improve from 0.05146\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0495 - accuracy: 0.9859 - val_loss: 0.0525 - val_accuracy: 0.9846\n",
      "Epoch 90/100\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.0474 - accuracy: 0.9856\n",
      "Epoch 90: val_loss did not improve from 0.05146\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0468 - accuracy: 0.9859 - val_loss: 0.0542 - val_accuracy: 0.9795\n",
      "Epoch 91/100\n",
      "74/78 [===========================>..] - ETA: 0s - loss: 0.0496 - accuracy: 0.9873\n",
      "Epoch 91: val_loss did not improve from 0.05146\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0496 - accuracy: 0.9874 - val_loss: 0.0773 - val_accuracy: 0.9754\n",
      "Epoch 92/100\n",
      "65/78 [========================>.....] - ETA: 0s - loss: 0.0479 - accuracy: 0.9858\n",
      "Epoch 92: val_loss did not improve from 0.05146\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0482 - accuracy: 0.9856 - val_loss: 0.0524 - val_accuracy: 0.9826\n",
      "Epoch 93/100\n",
      "74/78 [===========================>..] - ETA: 0s - loss: 0.0464 - accuracy: 0.9873\n",
      "Epoch 93: val_loss improved from 0.05146 to 0.05117, saving model to ./deep_model/model_check\\93-0.051173.hdf5\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0454 - accuracy: 0.9877 - val_loss: 0.0512 - val_accuracy: 0.9826\n",
      "Epoch 94/100\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0590 - accuracy: 0.9829\n",
      "Epoch 94: val_loss did not improve from 0.05117\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0567 - accuracy: 0.9838 - val_loss: 0.0582 - val_accuracy: 0.9805\n",
      "Epoch 95/100\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0433 - accuracy: 0.9885\n",
      "Epoch 95: val_loss did not improve from 0.05117\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0460 - accuracy: 0.9867 - val_loss: 0.0593 - val_accuracy: 0.9795\n",
      "Epoch 96/100\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0505 - accuracy: 0.9863\n",
      "Epoch 96: val_loss did not improve from 0.05117\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0500 - accuracy: 0.9864 - val_loss: 0.0613 - val_accuracy: 0.9774\n",
      "Epoch 97/100\n",
      "76/78 [============================>.] - ETA: 0s - loss: 0.0441 - accuracy: 0.9868\n",
      "Epoch 97: val_loss improved from 0.05117 to 0.05012, saving model to ./deep_model/model_check\\97-0.050119.hdf5\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0459 - accuracy: 0.9864 - val_loss: 0.0501 - val_accuracy: 0.9805\n",
      "Epoch 98/100\n",
      "72/78 [==========================>...] - ETA: 0s - loss: 0.0440 - accuracy: 0.9881\n",
      "Epoch 98: val_loss did not improve from 0.05012\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0468 - accuracy: 0.9872 - val_loss: 0.0624 - val_accuracy: 0.9754\n",
      "Epoch 99/100\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.0524 - accuracy: 0.9855\n",
      "Epoch 99: val_loss did not improve from 0.05012\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0512 - accuracy: 0.9856 - val_loss: 0.0671 - val_accuracy: 0.9774\n",
      "Epoch 100/100\n",
      "65/78 [========================>.....] - ETA: 0s - loss: 0.0498 - accuracy: 0.9855\n",
      "Epoch 100: val_loss did not improve from 0.05012\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0495 - accuracy: 0.9849 - val_loss: 0.0619 - val_accuracy: 0.9785\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, validation_split=0.2, batch_size=50, epochs=100, \\\n",
    "                    callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "faedf089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.5099935531616211,\n",
       "  0.19833047688007355,\n",
       "  0.18308213353157043,\n",
       "  0.1744088977575302,\n",
       "  0.17236050963401794,\n",
       "  0.16001972556114197,\n",
       "  0.15003879368305206,\n",
       "  0.1371074616909027,\n",
       "  0.13077016174793243,\n",
       "  0.1214529499411583,\n",
       "  0.117008276283741,\n",
       "  0.11013554036617279,\n",
       "  0.104842908680439,\n",
       "  0.10028354078531265,\n",
       "  0.09791398793458939,\n",
       "  0.10995856672525406,\n",
       "  0.09213206171989441,\n",
       "  0.0796702429652214,\n",
       "  0.08824791014194489,\n",
       "  0.09258729219436646,\n",
       "  0.08166752010583878,\n",
       "  0.07394925504922867,\n",
       "  0.07505107671022415,\n",
       "  0.0746365562081337,\n",
       "  0.07016479223966599,\n",
       "  0.07521798461675644,\n",
       "  0.07308253645896912,\n",
       "  0.06402964144945145,\n",
       "  0.07266312837600708,\n",
       "  0.06776205450296402,\n",
       "  0.06516791880130768,\n",
       "  0.07044778764247894,\n",
       "  0.06396353244781494,\n",
       "  0.07218305766582489,\n",
       "  0.057899653911590576,\n",
       "  0.06297057867050171,\n",
       "  0.07766953110694885,\n",
       "  0.06043368577957153,\n",
       "  0.07364320755004883,\n",
       "  0.06240598112344742,\n",
       "  0.05505530908703804,\n",
       "  0.05745019391179085,\n",
       "  0.05593562498688698,\n",
       "  0.06739618629217148,\n",
       "  0.06020018458366394,\n",
       "  0.06997401267290115,\n",
       "  0.05445012077689171,\n",
       "  0.054036349058151245,\n",
       "  0.06878723949193954,\n",
       "  0.06791850179433823,\n",
       "  0.0533214695751667,\n",
       "  0.052276283502578735,\n",
       "  0.05886625871062279,\n",
       "  0.0566706582903862,\n",
       "  0.06330657750368118,\n",
       "  0.07236897200345993,\n",
       "  0.05204761028289795,\n",
       "  0.06270786374807358,\n",
       "  0.055943526327610016,\n",
       "  0.051738038659095764,\n",
       "  0.05218670517206192,\n",
       "  0.05272616818547249,\n",
       "  0.06274490803480148,\n",
       "  0.05676243081688881,\n",
       "  0.05554945394396782,\n",
       "  0.05728181451559067,\n",
       "  0.054923731833696365,\n",
       "  0.051191143691539764,\n",
       "  0.050330329686403275,\n",
       "  0.051585517823696136,\n",
       "  0.04693000018596649,\n",
       "  0.05093035101890564,\n",
       "  0.045167259871959686,\n",
       "  0.04459495469927788,\n",
       "  0.048667971044778824,\n",
       "  0.06480620801448822,\n",
       "  0.05164969712495804,\n",
       "  0.0476524718105793,\n",
       "  0.05125473439693451,\n",
       "  0.05085482820868492,\n",
       "  0.045937035232782364,\n",
       "  0.04965156316757202,\n",
       "  0.052708689123392105,\n",
       "  0.04929831996560097,\n",
       "  0.06436007469892502,\n",
       "  0.05091235786676407,\n",
       "  0.05109653249382973,\n",
       "  0.059048380702733994,\n",
       "  0.049456361681222916,\n",
       "  0.046768199652433395,\n",
       "  0.049628689885139465,\n",
       "  0.04824664443731308,\n",
       "  0.045394979417324066,\n",
       "  0.05668920651078224,\n",
       "  0.04596273973584175,\n",
       "  0.049980513751506805,\n",
       "  0.045910537242889404,\n",
       "  0.04676121100783348,\n",
       "  0.05121884122490883,\n",
       "  0.04945489764213562],\n",
       " 'accuracy': [0.8799076080322266,\n",
       "  0.9299461245536804,\n",
       "  0.9322555661201477,\n",
       "  0.935591459274292,\n",
       "  0.9399538040161133,\n",
       "  0.9448293447494507,\n",
       "  0.9497048854827881,\n",
       "  0.9517577886581421,\n",
       "  0.9553502798080444,\n",
       "  0.9581729769706726,\n",
       "  0.9579163193702698,\n",
       "  0.9625352621078491,\n",
       "  0.9645881652832031,\n",
       "  0.9643315076828003,\n",
       "  0.9666410088539124,\n",
       "  0.9640749096870422,\n",
       "  0.9707467555999756,\n",
       "  0.9740826487541199,\n",
       "  0.9692071080207825,\n",
       "  0.9697203040122986,\n",
       "  0.9743392467498779,\n",
       "  0.975622296333313,\n",
       "  0.9769052863121033,\n",
       "  0.9774185419082642,\n",
       "  0.9776751399040222,\n",
       "  0.974595844745636,\n",
       "  0.9779317378997803,\n",
       "  0.9792147874832153,\n",
       "  0.9751090407371521,\n",
       "  0.9810110330581665,\n",
       "  0.9781883358955383,\n",
       "  0.9781883358955383,\n",
       "  0.9810110330581665,\n",
       "  0.975878894329071,\n",
       "  0.9810110330581665,\n",
       "  0.9799845814704895,\n",
       "  0.9733127951622009,\n",
       "  0.9820374846458435,\n",
       "  0.9776751399040222,\n",
       "  0.9784449338912964,\n",
       "  0.9838337302207947,\n",
       "  0.9815242290496826,\n",
       "  0.9828072786331177,\n",
       "  0.9799845814704895,\n",
       "  0.9812676310539246,\n",
       "  0.9769052863121033,\n",
       "  0.9830638766288757,\n",
       "  0.9848601222038269,\n",
       "  0.9763920903205872,\n",
       "  0.9789581894874573,\n",
       "  0.9835771322250366,\n",
       "  0.9843469262123108,\n",
       "  0.9828072786331177,\n",
       "  0.9843469262123108,\n",
       "  0.9799845814704895,\n",
       "  0.9774185419082642,\n",
       "  0.9846035242080688,\n",
       "  0.9794713854789734,\n",
       "  0.9828072786331177,\n",
       "  0.9846035242080688,\n",
       "  0.9843469262123108,\n",
       "  0.9853733777999878,\n",
       "  0.9812676310539246,\n",
       "  0.9843469262123108,\n",
       "  0.9840903282165527,\n",
       "  0.9835771322250366,\n",
       "  0.986143171787262,\n",
       "  0.9851167798042297,\n",
       "  0.9853733777999878,\n",
       "  0.9843469262123108,\n",
       "  0.987169623374939,\n",
       "  0.9851167798042297,\n",
       "  0.9869130253791809,\n",
       "  0.9876828193664551,\n",
       "  0.9853733777999878,\n",
       "  0.9792147874832153,\n",
       "  0.9866564273834229,\n",
       "  0.9858865737915039,\n",
       "  0.9853733777999878,\n",
       "  0.986143171787262,\n",
       "  0.9876828193664551,\n",
       "  0.986143171787262,\n",
       "  0.9848601222038269,\n",
       "  0.98639976978302,\n",
       "  0.9810110330581665,\n",
       "  0.9851167798042297,\n",
       "  0.987169623374939,\n",
       "  0.9838337302207947,\n",
       "  0.9858865737915039,\n",
       "  0.9858865737915039,\n",
       "  0.987426221370697,\n",
       "  0.9856299757957458,\n",
       "  0.9876828193664551,\n",
       "  0.9838337302207947,\n",
       "  0.9866564273834229,\n",
       "  0.98639976978302,\n",
       "  0.98639976978302,\n",
       "  0.987169623374939,\n",
       "  0.9856299757957458,\n",
       "  0.9848601222038269],\n",
       " 'val_loss': [0.22453081607818604,\n",
       "  0.18798835575580597,\n",
       "  0.1834016889333725,\n",
       "  0.1727110892534256,\n",
       "  0.16899240016937256,\n",
       "  0.14963172376155853,\n",
       "  0.14099863171577454,\n",
       "  0.13417984545230865,\n",
       "  0.13639980554580688,\n",
       "  0.11584067344665527,\n",
       "  0.12990303337574005,\n",
       "  0.10408449172973633,\n",
       "  0.11423055827617645,\n",
       "  0.10926472395658493,\n",
       "  0.09656254202127457,\n",
       "  0.14895449578762054,\n",
       "  0.1130533516407013,\n",
       "  0.08544988930225372,\n",
       "  0.08389449864625931,\n",
       "  0.10356418043375015,\n",
       "  0.07855768501758575,\n",
       "  0.07457159459590912,\n",
       "  0.12512700259685516,\n",
       "  0.07510517537593842,\n",
       "  0.06943407654762268,\n",
       "  0.10145963728427887,\n",
       "  0.07057911157608032,\n",
       "  0.07135957479476929,\n",
       "  0.10124096274375916,\n",
       "  0.12820960581302643,\n",
       "  0.06348257511854172,\n",
       "  0.10083217173814774,\n",
       "  0.06491529196500778,\n",
       "  0.07030770182609558,\n",
       "  0.11537253111600876,\n",
       "  0.06151081249117851,\n",
       "  0.07868707925081253,\n",
       "  0.05833336338400841,\n",
       "  0.05872483551502228,\n",
       "  0.05747020244598389,\n",
       "  0.06871841847896576,\n",
       "  0.05729024484753609,\n",
       "  0.06968457996845245,\n",
       "  0.059041526168584824,\n",
       "  0.07168219983577728,\n",
       "  0.0562385693192482,\n",
       "  0.06352448463439941,\n",
       "  0.05803707242012024,\n",
       "  0.07607153058052063,\n",
       "  0.07393229007720947,\n",
       "  0.054291777312755585,\n",
       "  0.09288132935762405,\n",
       "  0.05927729234099388,\n",
       "  0.07040542364120483,\n",
       "  0.11266496032476425,\n",
       "  0.060887180268764496,\n",
       "  0.07860630750656128,\n",
       "  0.054880812764167786,\n",
       "  0.08309271186590195,\n",
       "  0.06642492860555649,\n",
       "  0.06547704339027405,\n",
       "  0.09003552794456482,\n",
       "  0.05935442075133324,\n",
       "  0.057070642709732056,\n",
       "  0.08010423928499222,\n",
       "  0.055642299354076385,\n",
       "  0.08210744708776474,\n",
       "  0.05174090340733528,\n",
       "  0.05335735157132149,\n",
       "  0.055233411490917206,\n",
       "  0.059722885489463806,\n",
       "  0.0514577254652977,\n",
       "  0.05168227478861809,\n",
       "  0.05603182688355446,\n",
       "  0.06968990713357925,\n",
       "  0.05577902868390083,\n",
       "  0.05588553473353386,\n",
       "  0.07049446552991867,\n",
       "  0.053233660757541656,\n",
       "  0.054966531693935394,\n",
       "  0.11150269955396652,\n",
       "  0.05341193452477455,\n",
       "  0.06634315103292465,\n",
       "  0.0560169517993927,\n",
       "  0.08081874251365662,\n",
       "  0.06559739261865616,\n",
       "  0.06373284012079239,\n",
       "  0.05245108902454376,\n",
       "  0.05253603309392929,\n",
       "  0.05420060083270073,\n",
       "  0.07732661813497543,\n",
       "  0.052422165870666504,\n",
       "  0.051173485815525055,\n",
       "  0.05815323442220688,\n",
       "  0.05929433926939964,\n",
       "  0.06129017844796181,\n",
       "  0.05011859908699989,\n",
       "  0.06243827939033508,\n",
       "  0.06714346259832382,\n",
       "  0.06187675520777702],\n",
       " 'val_accuracy': [0.9302564263343811,\n",
       "  0.9364102482795715,\n",
       "  0.9374359250068665,\n",
       "  0.9394871592521667,\n",
       "  0.9405128359794617,\n",
       "  0.9435897469520569,\n",
       "  0.945641040802002,\n",
       "  0.9466666579246521,\n",
       "  0.9435897469520569,\n",
       "  0.9497435688972473,\n",
       "  0.945641040802002,\n",
       "  0.9548717737197876,\n",
       "  0.9507692456245422,\n",
       "  0.9538461565971375,\n",
       "  0.964102566242218,\n",
       "  0.945641040802002,\n",
       "  0.9548717737197876,\n",
       "  0.9671794772148132,\n",
       "  0.9682051539421082,\n",
       "  0.964102566242218,\n",
       "  0.9723076820373535,\n",
       "  0.9733333587646484,\n",
       "  0.9569230675697327,\n",
       "  0.9712820649147034,\n",
       "  0.9764102697372437,\n",
       "  0.9651281833648682,\n",
       "  0.9733333587646484,\n",
       "  0.9723076820373535,\n",
       "  0.964102566242218,\n",
       "  0.9599999785423279,\n",
       "  0.9774358868598938,\n",
       "  0.9671794772148132,\n",
       "  0.9753845930099487,\n",
       "  0.9774358868598938,\n",
       "  0.962051272392273,\n",
       "  0.9784615635871887,\n",
       "  0.9733333587646484,\n",
       "  0.9794871807098389,\n",
       "  0.980512797832489,\n",
       "  0.9774358868598938,\n",
       "  0.9764102697372437,\n",
       "  0.980512797832489,\n",
       "  0.9764102697372437,\n",
       "  0.980512797832489,\n",
       "  0.9723076820373535,\n",
       "  0.9794871807098389,\n",
       "  0.9784615635871887,\n",
       "  0.9794871807098389,\n",
       "  0.9743589758872986,\n",
       "  0.9733333587646484,\n",
       "  0.9794871807098389,\n",
       "  0.9682051539421082,\n",
       "  0.9815384745597839,\n",
       "  0.9764102697372437,\n",
       "  0.9610256552696228,\n",
       "  0.9774358868598938,\n",
       "  0.9733333587646484,\n",
       "  0.9784615635871887,\n",
       "  0.9682051539421082,\n",
       "  0.9774358868598938,\n",
       "  0.980512797832489,\n",
       "  0.9712820649147034,\n",
       "  0.9815384745597839,\n",
       "  0.9753845930099487,\n",
       "  0.9733333587646484,\n",
       "  0.9794871807098389,\n",
       "  0.9733333587646484,\n",
       "  0.983589768409729,\n",
       "  0.9825640916824341,\n",
       "  0.9846153855323792,\n",
       "  0.9784615635871887,\n",
       "  0.9815384745597839,\n",
       "  0.9825640916824341,\n",
       "  0.9794871807098389,\n",
       "  0.9753845930099487,\n",
       "  0.980512797832489,\n",
       "  0.9825640916824341,\n",
       "  0.9753845930099487,\n",
       "  0.9825640916824341,\n",
       "  0.980512797832489,\n",
       "  0.9671794772148132,\n",
       "  0.9794871807098389,\n",
       "  0.9784615635871887,\n",
       "  0.9815384745597839,\n",
       "  0.9702563881874084,\n",
       "  0.9784615635871887,\n",
       "  0.9794871807098389,\n",
       "  0.983589768409729,\n",
       "  0.9846153855323792,\n",
       "  0.9794871807098389,\n",
       "  0.9753845930099487,\n",
       "  0.9825640916824341,\n",
       "  0.9825640916824341,\n",
       "  0.980512797832489,\n",
       "  0.9794871807098389,\n",
       "  0.9774358868598938,\n",
       "  0.980512797832489,\n",
       "  0.9753845930099487,\n",
       "  0.9774358868598938,\n",
       "  0.9784615635871887]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7095ead5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x240da440040>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAA1pUlEQVR4nO3dd3hUVf7H8feZSSOUhJAAIQk9NOkGpEgTQVBXrAi67rr2wtq2KLq/Lbqra9tixd4VG7KoICggUgQSOoQEAiGFAAkJ6T1zfn+cmWSSTCBAYryT7+t5eKbmzrlD8pkz33PuuUprjRBCCOuztXQDhBBCNA0JdCGE8BIS6EII4SUk0IUQwktIoAshhJfwaakXDg0N1T179myplxdCCEvasmXLca11mKfHWizQe/bsSVxcXEu9vBBCWJJSKqWhx6TkIoQQXkICXQghvIQEuhBCeAkJdCGE8BIS6EII4SUk0IUQwktIoAshhJewXKAnHi3g2RWJHC8sa+mmCCHEz4rlAv1AViHPr0qSQBdCiDoaFehKqRlKqUSlVJJS6iEPj09WSuUppbY7//256Ztq+NpNkysq5cQcQgjh7pSH/iul7MCLwDQgHYhVSi3RWsfXeeparfWlzdDGWnztCoAKh6O5X0oIISylMT300UCS1vqg1rocWAjMat5mNcyvuocugS6EEO4aE+gRQJrb7XTnfXWNVUrtUEotU0qd42lDSqnblFJxSqm4rKysM2gu+Po4A71KSi5CCOGuMYGuPNxXN023Aj201sOA54HFnjaktX5Vax2jtY4JC/O4+uMpVdfQq6SHLoQQ7hoT6OlAlNvtSCDD/Qla63ytdaHz+lLAVykV2mStdOOqoZdLoAshRC2NCfRYIFop1Usp5QfMAZa4P0Ep1VUppZzXRzu3m93UjQW3GroEuhBC1HLKWS5a60ql1DxgOWAH3tRa71FK3eF8fAFwNXCnUqoSKAHmaK2bpcgtJRchhPCsUWcscpZRlta5b4Hb9ReAF5q2aZ5VD4rKPHQhhKjFckeKSg1dCCE8s1ygSw1dCCE8s1ygSw1dCCE8s3CgSw1dCCHcWTDQnTV0OfRfCCFqsVygK6XwsSkpuQghRB2WC3QwZRcJdCGEqM2iga6khi6EEHVYMtD9fGwyD10IIeqwZKD72m1USqALIUQtlg10KbkIIURtFg10JSUXIYSow6KBbpNT0AkhRB2WDHQ/H5m2KIQQdVky0KWGLoQQ9Vk00KWGLoQQdVk00KXkIoQQdVky0P0k0IUQoh5LBrqZ5SI1dCGEcGfNQJdZLkIIUY81A10GRYUQoh5LBrrU0IUQoj5LBrqPLJ8rhBD1WDLQ5dB/IYSoz5KB7meX9dCFEKIuSwa6HFgkhBD1WTbQHRqqHFJHF0IIF2sGuo8CkF66EEK4sWSg+9lNsyXQhRCihiUD3bc60KXkIoQQLhYPdOmhCyGEi0UD3dTQy2UuuhBCVLNkoPv5SA9dCCHqsmSgSw1dCCHqa1SgK6VmKKUSlVJJSqmHTvK8UUqpKqXU1U3XxPqkhi6EEPWdMtCVUnbgRWAmMAiYq5Qa1MDzngSWN3Uj66quoUugCyFEtcb00EcDSVrrg1rrcmAhMMvD834LfA5kNmH7PKqehy6DokIIUa0xgR4BpLndTnfeV00pFQFcASw42YaUUrcppeKUUnFZWVmn29Zqvj5SQxdCiLoaE+jKw311k/Q/wINa66qTbUhr/arWOkZrHRMWFtbIJtYnNXQhhKjPpxHPSQei3G5HAhl1nhMDLFRKAYQCFyulKrXWi5uikXX52KSGLoQQdTUm0GOBaKVUL+AwMAe4zv0JWuterutKqbeBr5orzEHmoQshhCenDHStdaVSah5m9oodeFNrvUcpdYfz8ZPWzZuDlFyEEKK+xvTQ0VovBZbWuc9jkGutbzz7Zp2ca9piRaUMigohhIsljxR1TVuUGroQQtSwZKC7Si6VEuhCCFHNmoEu89CFEKIeawa6HPovhBD1WDPQbTLLRQgh6rJkoNtsCh+bkkAXQgg3lgx0MAOjUkMXQogaFg50JaegE0IIN5YNdD8fm5RchBDCjWUD3ZRcJNCFEMLF4oEuNXQhhHCxcKArmYcuhBBuLBzoNjkFnRBCuLFsoMugqBBC1GbZQDcHFkkNXQghXCwb6L52m9TQhRDCjWUDXUouQghRm2UDXeahCyFEbRYOdEWl1NCFEKKahQNdauhCCOHOsoHuJyUXIYSoxbKBbg4skpKLEEK4WDfQfeQEF0II4c66gS41dCGEqMWygS41dCGEqM2ygS7L5wohRG2WDvQqh6bKIaEuhBBg5UD3UQBSdhFCCCfLBrqf3TRdAl0IIQzLBrpvdaBLyUUIIcArAl166EIIARYOdB+7qaGXy2nohBACsHCgSw1dCCFqa1SgK6VmKKUSlVJJSqmHPDw+Sym1Uym1XSkVp5Q6v+mbWpvU0IUQojafUz1BKWUHXgSmAelArFJqidY63u1pK4ElWmutlBoKfAIMaI4Gu/jaZdqiEEK4a0wPfTSQpLU+qLUuBxYCs9yfoLUu1Fq7usptgWbvNvv6SMlFCCHcNSbQI4A0t9vpzvtqUUpdoZRKAL4GbvK0IaXUbc6STFxWVtaZtLean5RchBCilsYEuvJwX70U1Vp/obUeAFwOPOZpQ1rrV7XWMVrrmLCwsNNqaF0ybVEIIWprTKCnA1FutyOBjIaerLX+AeijlAo9y7adlKuGLkvoCiGE0ZhAjwWilVK9lFJ+wBxgifsTlFJ9lVLKeX0k4AdkN3Vj3VX30GUeuhBCAI2Y5aK1rlRKzQOWA3bgTa31HqXUHc7HFwBXAb9SSlUAJcC1boOkzcLPR2roQgjh7pSBDqC1XgosrXPfArfrTwJPNm3TTk5q6EIIUZtljxSVGroQQtRm2UCXQ/+FEKI2ywa6DIoKIURt1g10GRQVQoharBvoUkMXQoharBvoNqmhCyGEO8sGus2msNuUBLoQQjhZNtDBlF2khi6EEIbFA90mp6ATQggnSwe6n90mJRchhHCydKD72m1USslFCCEAqwe6jwyKCiGEi7UD3W6TeehCCOFk6UCXGroQQtSwdKD72m0ybVEIIZwsHuhSQxdCCBeLB7rMQxdCCBdLB7qfj9TQhRDCxdKBLjV0IYSoYfFAlxq6EEK4WDzQZR66EEK4WDrQZR66EELUsHSg+9ptVFRKDV0IIcDige4jNXQhhKhm6UCXGroQQtSwXqAX58C+5VBVIfPQhRDCjfUCPWklfDgbshLkFHRCCOHGeoHebbi5zNiOr91GlUPjcEioCyGE9QI9pA/4tYcjO/C1m+ZXOKTsIoQQ1gt0mw3Ch8KR7fi5Al3KLkIIYcFABwgfBkd346eqAKiQFReFEMKqgT4cKkvoVJYCIDNdhBACqwa6c2A0rCABQOaiCyEEVg30Tn3Bty2h+fGA1NCFEAIaGehKqRlKqUSlVJJS6iEPj1+vlNrp/LdBKTWs6ZvqxmaH8KF0zN8LSMlFCCGgEYGulLIDLwIzgUHAXKXUoDpPSwYmaa2HAo8BrzZ1Q+sJH0ZQ7l5sOOQ0dEIIQeN66KOBJK31Qa11ObAQmOX+BK31Bq31CefNjUBk0zbTg/Dh2KtK6K0ypIcuhBA0LtAjgDS32+nO+xpyM7DM0wNKqduUUnFKqbisrKzGt9IT58DoEJVMZkHZ2W1LCCG8QGMCXXm4z+MopFJqCibQH/T0uNb6Va11jNY6JiwsrPGt9CS0H9qnDaMD0nhnw6Gz25YQQniBxgR6OhDldjsSyKj7JKXUUOB1YJbWOrtpmncSNjuq6xAmdzjMhgPZbEs9ceqfEUIIL9aYQI8FopVSvZRSfsAcYIn7E5RS3YFFwA1a631N38wGdBtO1+J9dGxj56XvD/xkLyuEED9Hpwx0rXUlMA9YDuwFPtFa71FK3aGUusP5tD8DnYCXlFLblVJxzdZid+HDUeVF/GVgBt/GHyPxaMFP8rJCCPFzpLRumYNyYmJidFzcWeZ+YRa8cSGcOMSn+gK29X+Ax6+b0DQNFEKInyGl1BatdYynx6x5pKhLuzC480cYfy9XqTXcn3g9aQmxLd0qIYRoEdYOdAC/QJj2KLm/XAFKUf7JLRQUFbV0q4QQ4idn/UB3Cukbw/HJT9PHcYjvX/k9VXIWIyFEK+M1gQ4wcPJsDkTMYmbeQt755LOWbo4QQvykvCrQAfrc8DxFfqFMjP8z17+8ihdXJ7H7cB4tNfgrhBA/Fa8LdAKCaHfNy/S1ZfDvrNtI+W4Blz//vcxTF0J4Pe8LdMDe70L41f/oHN6dp3xfY0O7h1i+8jvSTxS3dNOEEKLZeGWgA9B7MtyyEuZ+TKhPCXfbP+fxpXtbulVCCNFsvDfQAZSC/jOwDZ/LhbatbNi1nw1Jx1u6VUII0Sy8O9Bdhl+HXVfy6/Zx/PXLPVTK+ulCCC/UOgK96xDoOoSb2v3IvmOFvLX+UEu3SAghmlzrCHSAYdcRdGI3v+pTzNPLE9mTkdfSLRJCiCbVegJ9yDVg82F+t20EB/ry24+2UVxe2dKtEkKIJtN6Ar1dGERfRJv4T/nPNYNJPl7Eo1/Gt3SrhBCiybSeQAcYPheKMhnHDu6c1IeFsWks3XXkp3v9UinzCCGaT+sK9OiLoH04LHuQ+8eHMiQiiEe/jKekvKr5XzsrEZ7sBakbm/+1hBCtUusKdB8/mP0u5Gfg+9mv+MvFfTmaX8praw82/2sf2QG6Cg6tbf7XEkK0Sq0r0AGiRsOsFyBlPTG7H2PmOV1YsOYAmfmlzfu6OcnmMmN7876OEKLVan2BDjB0Nkz8A2x7nyfCllNR5eDZFc18busTrkDf1ryvI4RotVpnoANMfhiGXkvwxqd4O2oZn2xJJT4jv/leL8dZ1sk/DIWZzfc6QohWy6elG9BibDa4/GXwbcP4LW/zhP8xHv2wnAXnHSc4bSVExMCkPzTd6+UkQ0hvE+wZ26Hf9KbbthBC0Jp76AA2O1z6Hxj3W+awnIWFvyF45R9wHPge1j4LZQVN8zplhVCUCYMuBxQc2d402xVCCDett4fuohRMeww6n0NmRjL3b+uKrSyf92x/gYSlMOzamudqbf7ZTvNz0FU/7zoEQqOlji6EaBatu4fuohQMn0vnix/m2XnXcbzjcA7rMIq2fFj7eSv+BC+PM6F+OlwzXEJ6QfhwawR66iZw/ATz84UQTUYCvY6uQQG8edN5rLCdT0DqDxTlOI8kPXEINr0CWXshK+H0NurqoXfsBd1GQMERKDjapO1uUmmx8OZ0SFzW0i0RQpwGCXQPwoPaMOLiW7Hj4KuFL5kTTK952vTkAZK+O70N5hyENiHQJtgEOjTdfPRv5sMnv26abbm49i/nJzjgSgjRZCTQGzB81Hiy2/alz9Fv+GDpKvSOD2HUrRA2EJJWnt7GXDNcwNTRm3Jg9OAaSF7TNNuq3ub35jI/o2m3K4RoVhLoJxEy5npibPuI3vQwZfiSPOA26DsVUtZDeVHjN3Qi2dTPAfzbQVj/pqmja2160SUnoDjn7LcHUJoP6bHmen5602xTCPGTkEA/CTX4KgDOsyXwvp7J9NfiWVQwEKrK4dD6xm2kshzy0k393KXbiKYJ9IIjUFlirjdVeeTQOrPmjH8H6aELYTES6CfTsQdEjQH/Dlx+15PMHBzO/Li2lCt/HEnf1jyvJNcMIDo8nKs0NxW0o6bkAmamS+ExyD/LpXuzD3i+fjYOrgbfQOg3A/IOn9k2VvwJ1j/XNO0RQjSaBPqpXLEAfv0loZ278t85w7l1yiDWVQ7k+LalVFQ5TIh/eiN8NAfevQxOpNT++RNuUxZdqgdGt55d29x75TlNFejfQ49xpr2Fx8w3jNO1/UPY80XTtMcq8o/AF3eeXilO1HA4zEwycVYk0E8lpBd0Gw6AUorfX9Qf3/7T6FyRzp/e/JKSdS+YXu3QOWbmysvjYdsHNT/vDN3itlH8+9t95JVUQPgw8Akw5Y2zkXMA7H7QIbJpeuh5h+H4Pug9BTpEABoKT3N6ZdFxKM6u+SBrLRK+gh0fQnpcS7fEmvYsgufP/XlP57UACfQzMGHmXACGpb6DfdXfyO0+zfTk71xvwv9/d7lN/UsG37b8a0Mu/125n3c3HALfAIg6z8xQORvZB6BjT3P0aVP00A+uNpd9pkBQhLl+umWXrERzWXLClKJai8y95lKmep6ZzHhwVEJ2Uku3xNIaFehKqRlKqUSlVJJS6iEPjw9QSv2olCpTSv2+6Zv5MxPSG4J7cJ19Jfm0Z1rS1bz4/QEqO0TBLxeZx5c/AlWVcCKZ0g7deevHFGwKPtqcSpVDQ+9JkLkHCrPOvB05yRDSBzr1geyDp38Ea10HVkPbztB5kLOHjlkd8nQcT6y53pq+QrsONpNAPzOuUmVuasu2w+JOGehKKTvwIjATGATMVUoNqvO0HOAe4Jkmb+HPkVIQPQ2ANrNfY/Q5/Xh6eSKXv7Se3cdKYPrfzR/4lrfQOQfZUdiR9gE+PHb5YDLySvk+MRN6TTbbamgOudbw2U2mHu2Jw2HCI6Q3dOoLZXmm3HGmHA5TP+892ezfmQZ6ltu68q0l0LU2PUxofaWmpuIKcgn0s9KYHvpoIElrfVBrXQ4sBGa5P0Frnam1jgUqmqGNP0+T58ONX9N20DReuG4EL1w3gqN5ZVz2wjr+mtiD4ojxsPpxHDnJbCsK4cEZA5gdE0VYe38+2JRqSjP+QQ0HesY22P05bH7V8+OuKYudepteOjRcdtnzhantn2yAMzMeio+bcgtAQAczddG95KK1mcFyZEfD2zmeWNOe1hLohcdMiQlq1u0RpydXeuhNoTGBHgGkud1Od9532pRStyml4pRScVlZZ1Fq+DloGwo9zwfMYOmlQ7ux8neTmDO6O+9sTOGqg5fiKDmB3VGBI7gn18ZE4Wu3cW1MFKsTM0nPKzM/31AdfcdCc5mxzfMJMVxf7V0lF2h4YDT+f3Bs98mD2DVA23NCzX0dutXuoZ84BBuehy3vNLydrH0QcS4Edmo9vVVX77zLEPP/cralr9amosR8KIIE+llqTKArD/ed0W+s1vpVrXWM1jomLCzsTDbxsxbUxpfHrxjCxvlTmXvZxawOnAHAjAljsdnM2zj3vO4oYOHmNFPeyE2p36urLIfdn5llBsDz2jGu3nhIbwjuDsrecA89bbO5TP2x4canrDfbCY6qua9DRO1Ad30gNLRsQVmBObo0rJ8ZrG0tPXTXgOiAS6CiuCacROO4QtzuX9NTF2ekMYGeDrj9lRMJyCGEJ9GlQwC/GtuTqfNegimP0PvcmrMTRQS3YUr/znwcl0ZFD2dvuG7ZJek7KM4md/zD6HZdYf+K+i+S7ZyyGBQJdl9zEJSnHnpeek0op2703GCtIWUD9Bhf+/4O3WqXXI7udF7uhioP1bXjzvp5aH9zZKy3BHpJLhyLb/jxzHhoGwZRo8xtGRg9Pa5Ajxptft+qKlu2PRbWmECPBaKVUr2UUn7AHGBJ8zbLSwSGwKQ/go9frbuvH9OdrIIyvspoB+261iu7VG77gEKfjoz5BNbq4eiklfV/yXMOmtC02c3tkD6ee+iu3nnnc0wP3VM54Ph+Uz/vMa72/UGR5kxLrtq7q4deVVbTK3XnGhAN62966LlpnoPfatY8Ba9Phcoyz49nJkDngTVHAzdloFeWecd7eDKuD/6eE8yyEwXSXzxTpwx0rXUlMA9YDuwFPtFa71FK3aGUugNAKdVVKZUOPAD8SSmVrpTq0JwNt7JJ/TozOKIDTy3fR2XPCZD8Q/WyASvi9qITv+Hj0jEMjgrlwxMDUGX5kL659kZcM1xcGpq6mLbZHMo/6mYoyTHhXVeKc12aej1051BJQYbZ7pEd0H2suc/TWjTHE8HmY9rVsaf548zzggW+MraaUsrRXfUfczjMjKbOgyAoyln6asKxg/euhMV3Nt32fo5yU0y5JTLGeVvq6GeqUfPQtdZLtdb9tNZ9tNb/cN63QGu9wHn9qNY6UmvdQWsd7Lye35wNtzK7TfGXX5zDkbxSvisdaHrHR3fy5Y4MfvjiFXyp5Pyr5vHJ7WMpiZxABXYKdy+t2YBryqJrMBRMD72iqH79Nm2TGaTsNdHc9lRHT9kA7brU/oAAU3IB8zW44CgUZcGgWWZ2jqc6etY+0w67b81SB1YfGHU4TIkJPB8FmpcG5YWmh273NeMQTdVDLzgKKetMCc7TOkHe4kSKed869jS3JdDPmBwp2kJG9QzhsmHdeDyhCxqFfm0K0Ysu4n7/JTg6D6L/8PHYbIq/zh7LFscATmz/2pxoA5xTFkvr9NCd193r6OXFpu4dNdrMVQ/sVL+OrrXpofcYV3MCD5egSHOZn1FTbgkfDuFDG+6hh/Uz111/nFavo+emQLnzZOGHPQS6q/TU2XloRkjvxgd60fGTn+bPNXZScqL2AVveJjfFjAG5ft8k0M+YBHoLemjmADJVJ/4R9jSvcyX59hA6BtiwjbmrOlx7hbZF9ZtOVMVBvl7nXKfcfYaLi6e56BnbzOHUUeeZ7XUfW7+HnptqBk3rllugpoeen+4MdAVdB5vFxY7tqT2vvbLclBpC+5vb7buZQVurB7qrzBLU3XMP3TVlMWyAuQzpbd6HU01dLCuA/w439fmG7FsOfu3N9ZPNUGrI0V2w+K6ffw0+N9X00H38oX24GXtpjLx07/7mcgYk0FtQt+A23DW5L6+ndeM5PZuQO77C9sckGHlDrefFTLsWgE0rFvLDvqyaXrh7ySUoCmy+tXvorrp7pHP2RfcxpgTivgBSygZzWXdAFMC/vSmv5GeYnn6nvua+biPMmvCZbjM/cg6YmnmYM9BtNgjuUbueXHTcegfeHN1l6uIjrjfvXVF27ccz95rF0QKcQ0Yhvc1Ru+4nHDkWXz94Ujeanv/mV8087Loqy8xSDEOuNuWwlJME+tZ34aO59T9EYt+A7R/8vE9KXppvvoEE9zC3g7s3bupi3mHzgbj+P83ZOsuRQG9ht03szeyYSF69IYa+ndt7fI698wCqOvZmvv1Dtr//MJlJcWYQqUOk25N8TJnDvYeethk6RZvZNlAzoOledklZDwHBNXPe6wqKMH88R3aYUgtUrz5ZKyhci3KF9qu5r85c9MOvXUveSxeevMzwUyvNO/l6Okd3mX1yHkRWr+yStdfUz13qjh2k/Agvj4Vdn9b+OddU1ZIc2PlJ/ddNWW/GRPrNcH6zamDKKZiTlycurT3grTUccJ4q0fWh/XPkCu+O7oHeiJLL/uXgqIAfXzj7JYvjl8Daf53dNn4mJNBbWICvnaeuHsbYPp0afpJS2G/4HHv0Bdxj+4TOCe9zIiCCfyxL4Oa3Y7n2lR+55Z049pSFUpAcR2l+tvmDTtuEI2o0C9YcYPYrP/Lv3QE47AE43L++p2wwvXNbA78KHbqZUMtLM8v+gpkuGVBnYDQrEVC1Az3EORdda7KTtxGRG0dQ5XFStyyr9RJaawpKW6AsoDW8dwU80xdenQLfP1k/TI7uMueBDR8Oyla77FJVaQaCawV6namLsa+by711Zvom/2DKXF0Gw6YF9XvX+1aYJZZ7TTSBnpfqecZQTrI5Chhg3zdu9x+s2ZczKdf8VFyLcrn30PMbMRd93wrwa2eWat767tm1YfXjsPof5tuCxUmgW0VIb/yv/5Cs2UuIU0N4N28Y7/yYQvqJEhxaczi3hPfKJhFQkkn2v8exd9UHUJzNqwdD+eeyBLILy3jhh1Q2lfdm3+ZvSc3KN3XwnAPQfSyHjhdx94db2XiwTkmhQ4QJE4Cuzh66Uibg3Hroh5O2U94+EvwCa362Y08oM1+pDy39L6Xal0LdhqwN79d6iceX7mXcE6tIzS4++/cpPc4MBjdGxjY4vAUGXGqmW37/BLxzWU15pDjHjB90HWLOBdt5UO0e+olkMyffNSAKENwDjWJjXCyF2Rlm2QW7PxxYBRWl5jklJ+DIThPW591hSlfJP9RsQ2sTzr0mmvez+xhzv6deesJX5rJ9uKm5uxxYZS57nG9+7udaa3Z96LgHuqPSDPw3pKLUfMMZNsd8KG54/sxOxAKmRJm117yma/loC5NAt5iwQZM4Z/4arnzgJfY+OoPl90/k0zvGsezeCfzzkfnsmf4RfpQzcO3dACzN68Gz1wzjuwcmseVPFxIycCID9AEiXuwBL5u6eUH4WG56J5avdx5h7msbefTLeEornGWRoJqyTrJfH25+O5bPt6Q7B0bjobKMT9ftoSJ1C1uLw6iscgsO50yXvIOxDMxcyo7gC4nvOIX+Od+Tl296Q8nHi3hr/SEKyip5+ItdNTN5XDyVZyrLYeen9b9qp240BwCtegytNW+uSyYps6DhN3PLW2jfQApmPAe3fAtXvGJCOtVZonANiHYdYi4jzjUfAK5wzNhuLt166JU2P47bw8hIjid+6UumLDDtb2Yeuyu0D60HtAnsIdeY2UebFtS0KzvJtCPaeYRxl8FmcNRTTzvha7OGzLC55nHXGvQHVpuQHHE9lOaa0GoOB1bXvA8N2foeLLnH82O5Kaan7SoLBnd33n+SskvKOvN+Rl8EEx4wPfqdH5920wHz/oE5VsPTEdkWI4FuQW387ESFBGK31V9mZ/j4i2h/z3rSOowkyyecF++5lqvOjUQpRXCgH/1n3sXh6Ot4sXIWH3W+n8oblnDnKgdpOcW89ZtR3DCmB2+uT+bi/65l4eZUigO6AFDUphuXvBbP6sRMfvfpDpbldAVHBQnLXmb0t5cTZcvivZLzWbzd7Sg/54mxC755jEBVRrfp9xI27gbaqRK2fmuWBX56eQJ+PjbumRrNuqTjfLbFrayw/SN4uk/9RcV+fAEW3QLLHqSyysHCzalk5hfDsgfN41ve5oPV23n0q3jmL/JwMBBAaT561+es8pnAkH9uZMhflzNrVUfK7YGw4yPznLqBHjnK1NxzDphvAd8/bvbRrYf+xLIE9pWHEW0/So/kT8zRjzE3mdBKdB5LkPwD+LSBiBhzspOYm8w5aV0D2q7SSb+LzKXdxywrUHdgtDDTfIgNvNTU2nWVqZtXVZjX6HNBzbhJc9TR84+YwdhPbmi4h1yab1bo3PoOHPZwysUTKeaDxzVl1tVTdw/0uh/qrnJUz/Ohz1TzzXHdv89sbCbha/Pz/WbA/m+b55uMw2Fep6D51/iRQPdCAR27EfXAasLm7yKqU7vaD3bsQcT1L+N74f8xP3UUl36pWJd0nH9cMYQp/Tvz6KzBfHDLedhsiocW7eKur8wv4brCbgyNDGLNH6ZwxYgI/rE9AIABW/5CoF1TdsPXHOo6nedW7jfnWnW+FkBk4S6SA84h6pyx9Iq5iGxbJ/ziP2dLSg5Ldx3ltom9uW9qNKN6duTvX+8lq6DMhMWyB015Ysk9NX+s+RnwwzNmIHfbe6xc8h4PLdrFmy/+09T0z38AKorJXv08Ye39iT10gi0pOdSz61NURRHP5Y7nypERXDEiAu0byOLyUTh2L3bO4d9lpl+2DTU/4zqSMT3OhHnOQbjsueqlHRZvO8wb65IJ6NyHISTRxXGMo/2uM9Px+k41Qe1wwKG1poziWhIi5mZT8nl+JDw7wOxf50E1vVWA7uNMaca1TC+YDwG0WRQsMgbahJiyS3qcmUHT5wLzLal9ePPU0X94yhwPkZsKW98h4Wg+CUfr1KE3v2K+Idj9a8YT3LnmoLvUnYue8qP5UE90jrtobQZEe00krRAcGpjwO/Mh+8UdsGdx/ZlIDSnMNAfeDbjUfBsqPFazXlFTydoHb82EhdeZwfF9zfstQALdm9l9G3zojkm9uWRIOAlHC7h9Ym9mx9Ssvza+byjf3j+RJfPGM2qo6Z0G9Y7hg1vGEBUSyLPXDOOSCeex09GLNbbz4I61BPYZxwPT+pGaU8yiraaX7fAJpMDHDPYGjL/DbNxmJ6fXLxhVuYUH31tDaDt/bp3QG5tN8cSVQykpr+IvS3ajl/7e1Kcv+D8T1JteMT//3V9NvfPmbynvNJCR2//MtG5l3FT6LvG2fuwffB/r7KO50b6cL28fTsdAX95euRM+vNacRaqyHLRGb3mLZJ/eZLY/hyeuHMKjswbz1o2j+IpJ2CoKTY/KNSDqEtrPlD62vA0/vgjn3lh9BG7soRweWrST83qFMHz4SACydBAL853jDv0vhoIjrFn6oQlm15G7AB3C4TfLYMojJoS7DoHx99b+D+s+BtBsWe9WJ0/4yoR+l8FmTZ/o6aaXuX+FGcDtNbHm+IOUBtbxOVPZB8xgZMxN0H0c+odnuOPNddzwxmaKypwDmmUF5n2KvghG/BJ2fVZ7OqfWzjnoboHu42/WN8pNNR9+3zg/1P93t+nhZifBiUMc6zqJyc98z39X7oeBv4Dh15tvQJ/+Gp7u3bhZK4lLqf5A7HshoJqu7KK1+dawYLxZGuKiJ8wH64fXwDcPN7wu0FmSQG+llFI8O3sYb94Ywx9nDPD4+NDIYO6+6iKY9BBjrrynusRjsynmXzyI7OtWEH3P/wjrHA7ABQM6MywyiOdWJpGWU8wNb25ib3koBT4hhI+dU73tHlNuxE9VMbpkLfdPi6atvw8AfTu3494Lo3HsWYJK+Ao9eb7pfUVfBKv+bgJh58cwbh46NJq/+91DMAUsKPkdndUJ/l71K2Y8t57/lF5CEIV0TfqUW0eHcvOhB9D7vzWlmrdmQPxi1NFdvF4ymTun9MXfxyxw1qmdPwPGzCBdh1Ky8Q1zdKZ7oNvsEDEC0jaa0Jn2KAAr9x7jl69voltQG168fiR25/EBGzpczKKdWWZcIHo6Dmx02/w4AKlBMbXf8KhRZiG3y1+CG78yA35uDvgNoFzb2fT91/z9q3iqSvLNGaYG/KKmXNHvIjMNMvYNU85pE+x8w8eZ9XhONb9bazOvvjGli9WPm+MeJv0Rpv4fqvAo04q+JKugjNfWOmf4bH7VhPHkB2H0reYD2n1GSnGOWTbB/ZsI1MxF3/WJKbdN/IMZL1kyr7octSCjD1UOzas/HCSzqMK8bw8egpu/hX4zzawVT2vvuEv42nyYdDkH2oVBxMjGBbrW5ttZ7Ouw8Hp4bmT914pfbDof0dPh7s0w9i64ZSWMvg02vgjfzD/165wBCfRWLMDXzgUDunisxVez2WDK/JqTRruZMqAz3YLbVN9WSnHftH4czi3hgme/Z1tqLjkTHqPdrz+uteKkX8Rwctv25vf+i5kbfxe8MhFemQSLbudO3695KvBddjt68p/i6SasLnnGLMD/+c1UtQuH8x/g611HeDc5iJ197sBekgNDZvP7m39Jx0A/Zsy4rHr2w20pv2eQOsRbkY/C7HfNXO1Pb6RUBbChzZRa30wAbp3YlyV6Am0yfjTfBNwDHdiBmZb5Yef7WZ9ewSdxadz23hb6d23Pp3eMJbSdv6ntDpmNOu92UnOK2ZqaS1KhH3G6P9G2wxQSyJVfFLEzPbdR/09llVX89rME9qi+3Or7DTGb7mHVi/PMwV0DLql5Yp8L0MoOZXlo15mnwK2OfpKyS26qqYe/NMb0hk/Wmz+y06zXP+ZOaN8V3X0sW3xHMs/3S64Y1J5XfzhIVnY2bHgB+k4zg8mdB5oZN3Fv1Hxg5B4yl+4lFzCBnn0AVj5qBt8nP2w+PPevgDVPUR7Sn3fiHcw4pysVVQ6eX+k8sbTd1yxzcflL0KYjlYvnMefldfx1yR4Pb2qB8wPx0poPxOjpplzl6VSOWfvMUbevToEnouC5EfD178x7UZxt3jPXVMvyIlj+JzNYPftdaG/GofANgIufhrkLTUelGfg0y1ZFqzW5XxiT+4dRXFbFU1cPpWdo2/pPUorgi+bDhudAO0xvt6ocktdg27mQ9jYfvuv7JP9dlUxmYRXHC8voUXoVf7K/y/05VxH7r00UllUyNDKI4df9DXYOgAGXMjKwI7GPTEUpBV3vhw+uxqfwGAv7/J3HE3oy46opdLt9Dfkf385bhyP55dQhBPjaazUtrL0/DJ0Du78wdzgDXWvNP5cl8PHeUVzSsRufJUZRtmcTAOP7duKVG2Jo5/ymQWAIXPUaU0or8P/mKJ9vTWdPRj4TbKMYrfdi6zmOgGP+XPfaJp68aigXD+lq2tyAp75JJP5IPsVXvIDv8Y+ZuHMJgYWxHNdB3PZVFX277sDXbiP2UA5/q+zPWHs88zZ1JKo0gUuHhjM4fJA5biB1AwyfW3vjec4ZIj88bW73v9gMCgd3hykP129M0XEzthEQXF0W2ngwh8cLr+RL/z/xePmTjNW+5L31FGElOTDZ7Zzyo2+BT280ZaH+M2oGgYM9BPruz8z1q143nYpRt5rxgQMr+bHDJfjYbTw66xxC2/vx0eZUbjq/F71cv2uBIZRPfwK/L27hnIoPeSPlEs7p1oFr3D+8k74zv3MDL625L3qambqatBKGXVtz/5GdlL11GZXlZfhEnYv/8LlmqYdek8zR2vH/M6WeH1+A8++Ddf8x012veq1meWt3/Wd6+F9uGhLookkppXj7N6NP/cShs82/uopzUBXF/LZ9BKmf7eCjzal07RBA5KjbWd/1VwwrD0Wl53LoeBH/vGoodh9fGPmrWq8PmJro+Pugx3jODzsfEr5nwlOraefvQ5Xj9wQE2Fg7unv91weuvmgyW3dFM9CWzvIUP3oW5/Lejyl8vjWdG8YM5q+XXcsjlVWs23+cY/mlzB4VVV22cdc+wJcLB3bhw01mgO/ey66Hb98ncOB0PrtiHDe/E8vdH25lbO9O/PkXgxgY3gGtNWWVDk4Ul3OiqIKd6bm8sS6ZX4/twfjzBgOjCZz5NHu3rmFVUgH++X6sSsiktMLBuT06Uhg+l+yMjygKGs7raw+yYM0BJvcP47mwc+mQssGUCtJiTdno4JqaI4v7XwwznzKDkkvmwZonzXISrmUoHFWw5S1Y+Zgpk1z6n+qSzutrD5IROIDKYb+hTeJXzAiAggJN3qC5BEW6lZYGXGrqyN88CN/9xdSWbT6ee+jO51dEjiEhPY8B4e3xnfUipYvu4on95zJ7VCSdOwRwz9RoPt9ymGdWJPLidWbsorLKwbztPZldNZL5AZ+jg4fx0f9SGO03hB4cNfue+I2ZLhp1Xs3rho8wJypJXGp+N5WC9C043ruCnHIf5pY9RhRDeHfm6NofwINmmX37/gmz1tH6/5rpqJ6W02hmqt68359ITEyMjovzsNiREE4Oh+bg8SJ6h7atPoXfmVq7P4uNB7MpKK2ksLSSmUPCmTaoS4PP//jLr1m5MZYVVaOq77v/wn7cM7XvSXvTda3Yc5Tb3tvCpUPDeeG6kWbufmg02H2prHLwUWwaz65IJL+kgo6BfuSXVlBRVftvckDX9iy+e3y9bxPutNb12pVbXM7C2DQWrDnAnLLPech3YfVjlT6BHA4ayTb7UDbowUycMIVLhoSbbVRVUPn+NdgO/UB552EEqMqag6x6ToCLn4HOZtwlKbOQC/+1hnunRnP/NFOOyikqZ9JTq+kd1pZnrhlGdBezpEV5pYNdi55kQMJLpLcdxJGgERR2n8z0C6bj5+NW/c1KpOKT3/B+98d4eacms6CMiOA23DqhF/syC/k4No3vfz+ZqBBzENu/ViTy3KokbhzXE6VMm9buP85T0zoxe9PVNatluvi1M7OCzrvTfFNwt/hu2P6+OUF654FwLJ4s3Z7ZxQ8xdewoXl+XzD+uGMz159V8CJVXOvAryYQXRpvX8mkDv42rXtwuM7+U19clE9TGl9G9QhgaGeSxA9BYSqktWusYj49JoAvhWWlFFWk5xSQfL6J9gO/Jl2doQJVD83FsGpcMDSeojedZR7nF5by+NpkTxeV0aONL+wAfgtv40THQl+BAP4ZFBRHod+ZfpgtKK1i4Ko6gLc+zs7QLWxzRJOooHNgIDwrA38fGoexiLhzYhb/8YhCrEjJ5/bsd3FfxGuG2PPp260TnkGDTCx18VXXNubzSwe8+3cHyPUfZ8NAFZvzAacmODB5ZtIui8kquHRXFoPAOLFhzkMO5JXTp4E+VQ1NQWklZpYNB4R3417XDGNC1Axm5JbywOolP49KoqNJM6hfGtEFdWLztMHEpZsrmVSMjeXb2sFr794vn13EkrxQ/Hxv+PjZ+M74Xd0/pa76RHN3N3mNFPPtdEm3DunP++EnMHGqmR365I4OFsWlk5JYwoGt7YsIcTGcj/Wzp2LMSyC7VXJwylxumj+GuyX254c1NbEvNZfl9E6mocvDPZQmsiD/GJUPDeSxqGyErH4Cpf4EJD1Dl0Ly/MYVnlidSXFFFlcNkrb+Pjd9e0Jd5F0Sf0f+nBLoQAjDliKP5peQUldMjpC1BgeabwlvrD/Hst4mUVphjCMb0DuH2iX3493f72Jmexz1To7l7Sh/8fexorVkRf4wnlu7lUHYxd0zqw0Mz68+Uyikq57mV+3l/YwqVDs3wqGDuuzCaSf3Cqr9NrNhzlIe/2EVeSQUXDuzCyr2ZaDTXjori5vN719TFMVNDl2zP4M7JfWoNxjfWoq3pPL8qieTjRbTxtWNTUFReRf8u7RkcEUTC0Xz2HSugokoT2s6fq8+NZPG2wwQH+rJk3vn4+dg4nFvCRf/+gaA2vhzLL8Xfx8ZFg7uybNdRqhwO7h9SQXHH/hzOK2VXeh77MwuZEB3Ko7MGE9zGl9hDOWxKzuG8XiFMP6frGf0fSqALIU4pJbuI935MYWyfTlwwoDNKKUorqnjki9187jy2oL2/D4H+do7llxHduR2PXDKQyf07n3S7aTnFZBWWMSIq2GO5KqeonP9bvJtv449x5cgI5l3Ql8iOgR62dPa01mxNzeWLbelUOeCamMha7SqvdLB2fxYLY9NYlZCJQ2sW3TmOEd07Vm/jsy3pPPT5TuaMjuLeqf0Ia+/P0bxSnlmRyOdb01FA1w4BRHRswy/H9OCyYd1Oq0x3KhLoQogzprXm2/hj7DtWQHZRObnFFcT07Mi1MVH42Jtu5rOncYCWdCy/lKyCMgZHBNV7rKLKga+HfS8oraCNr71J35e6ThboMstFCHFSSimmn9P1jEsEp/M6PyddOgTQpUOAx8c8hTmYmU0tSQ4sEkIILyGBLoQQXkICXQghvIQEuhBCeAkJdCGE8BIS6EII4SUk0IUQwktIoAshhJdosSNFlVJZwClOodKgUMDDKvRerzXud2vcZ2id+90a9xlOf797aK3DPD3QYoF+NpRScQ0d+urNWuN+t8Z9hta5361xn6Fp91tKLkII4SUk0IUQwktYNdBfbekGtJDWuN+tcZ+hde53a9xnaML9tmQNXQghRH1W7aELIYSoQwJdCCG8hOUCXSk1QymVqJRKUko91NLtaQ5KqSil1Gql1F6l1B6l1L3O+0OUUt8qpfY7LzuealtWo5SyK6W2KaW+ct5uDfscrJT6TCmV4Pw/H9tK9vt+5+/3bqXUR0qpAG/bb6XUm0qpTKXUbrf7GtxHpdR8Z7YlKqUuOt3Xs1SgK6XswIvATGAQMFcpNahlW9UsKoHfaa0HAmOAu537+RCwUmsdDax03vY29wJ73W63hn3+L/CN1noAMAyz/16930qpCOAeIEZrPRiwA3Pwvv1+G5hR5z6P++j8G58DnOP8mZecmddolgp0YDSQpLU+qLUuBxYCs1q4TU1Oa31Ea73Veb0A8wcegdnXd5xPewe4vEUa2EyUUpHAJcDrbnd7+z53ACYCbwBorcu11rl4+X47+QBtlFI+QCCQgZftt9b6ByCnzt0N7eMsYKHWukxrnQwkYTKv0awW6BFAmtvtdOd9Xksp1RMYAWwCumitj4AJfeDkp1u3nv8AfwQcbvd5+z73BrKAt5ylpteVUm3x8v3WWh8GngFSgSNAntZ6BV6+304N7eNZ55vVAt3TWWS9dt6lUqod8Dlwn9Y6v6Xb05yUUpcCmVrrLS3dlp+YDzASeFlrPQIowvplhlNy1o1nAb2AbkBbpdQvW7ZVLe6s881qgZ4ORLndjsR8TfM6SilfTJh/oLVe5Lz7mFIq3Pl4OJDZUu1rBuOBy5RShzCltAuUUu/j3fsM5nc6XWu9yXn7M0zAe/t+Xwgka62ztNYVwCJgHN6/39DwPp51vlkt0GOBaKVUL6WUH2YAYUkLt6nJKaUUpqa6V2v9L7eHlgC/dl7/NfC/n7ptzUVrPV9rHam17on5f12ltf4lXrzPAFrro0CaUqq/866pQDxevt+YUssYpVSg8/d9KmasyNv3GxrexyXAHKWUv1KqFxANbD6tLWutLfUPuBjYBxwAHmnp9jTTPp6P+aq1E9ju/Hcx0AkzKr7feRnS0m1tpv2fDHzlvO71+wwMB+Kc/9+LgY6tZL//BiQAu4H3AH9v22/gI8wYQQWmB37zyfYReMSZbYnAzNN9PTn0XwghvITVSi5CCCEaIIEuhBBeQgJdCCG8hAS6EEJ4CQl0IYTwEhLoQgjhJSTQhRDCS/w/PLjmCFntf1gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "94070359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x240dbd31880>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72c4b23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5207311d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "39a421fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y_oh)\n",
    "model = Sequential()\n",
    "model.add(Dense(36, activation='relu', input_dim=12))\n",
    "model.add(Dense(18, activation='relu'))\n",
    "model.add(Dense(9, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "# model.summary()\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ea998c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "430af56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2a6d91bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = './deep_model/model_check/{epoch:02d}-{val_loss:4f}.hdf5'\n",
    "checkpointer = ModelCheckpoint(filepath = modelpath, monitor='val_loss', verbose=1, \\\n",
    "                               save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "573443c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "72/78 [==========================>...] - ETA: 0s - loss: 5.0051 - accuracy: 0.4917\n",
      "Epoch 1: val_loss improved from inf to 0.23266, saving model to ./deep_model/model_check\\01-0.232655.hdf5\n",
      "78/78 [==============================] - 1s 7ms/step - loss: 4.6413 - accuracy: 0.5245 - val_loss: 0.2327 - val_accuracy: 0.9210\n",
      "Epoch 2/200\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.2185 - accuracy: 0.9280\n",
      "Epoch 2: val_loss improved from 0.23266 to 0.20649, saving model to ./deep_model/model_check\\02-0.206489.hdf5\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 0.2159 - accuracy: 0.9284 - val_loss: 0.2065 - val_accuracy: 0.9262\n",
      "Epoch 3/200\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.1978 - accuracy: 0.9323\n",
      "Epoch 3: val_loss improved from 0.20649 to 0.19293, saving model to ./deep_model/model_check\\03-0.192933.hdf5\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.1962 - accuracy: 0.9328 - val_loss: 0.1929 - val_accuracy: 0.9354\n",
      "Epoch 4/200\n",
      "75/78 [===========================>..] - ETA: 0s - loss: 0.1852 - accuracy: 0.9355\n",
      "Epoch 4: val_loss improved from 0.19293 to 0.17674, saving model to ./deep_model/model_check\\04-0.176742.hdf5\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.1860 - accuracy: 0.9353 - val_loss: 0.1767 - val_accuracy: 0.9354\n",
      "Epoch 5/200\n",
      "65/78 [========================>.....] - ETA: 0s - loss: 0.1722 - accuracy: 0.9394\n",
      "Epoch 5: val_loss improved from 0.17674 to 0.16606, saving model to ./deep_model/model_check\\05-0.166059.hdf5\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.1760 - accuracy: 0.9371 - val_loss: 0.1661 - val_accuracy: 0.9385\n",
      "Epoch 6/200\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.1690 - accuracy: 0.9397\n",
      "Epoch 6: val_loss improved from 0.16606 to 0.15615, saving model to ./deep_model/model_check\\06-0.156155.hdf5\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.1669 - accuracy: 0.9407 - val_loss: 0.1562 - val_accuracy: 0.9395\n",
      "Epoch 7/200\n",
      "64/78 [=======================>......] - ETA: 0s - loss: 0.1697 - accuracy: 0.9400\n",
      "Epoch 7: val_loss improved from 0.15615 to 0.14974, saving model to ./deep_model/model_check\\07-0.149739.hdf5\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.1645 - accuracy: 0.9410 - val_loss: 0.1497 - val_accuracy: 0.9395\n",
      "Epoch 8/200\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.1503 - accuracy: 0.9458\n",
      "Epoch 8: val_loss improved from 0.14974 to 0.13484, saving model to ./deep_model/model_check\\08-0.134837.hdf5\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.1519 - accuracy: 0.9448 - val_loss: 0.1348 - val_accuracy: 0.9518\n",
      "Epoch 9/200\n",
      "64/78 [=======================>......] - ETA: 0s - loss: 0.1457 - accuracy: 0.9466\n",
      "Epoch 9: val_loss improved from 0.13484 to 0.12682, saving model to ./deep_model/model_check\\09-0.126821.hdf5\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.1438 - accuracy: 0.9474 - val_loss: 0.1268 - val_accuracy: 0.9477\n",
      "Epoch 10/200\n",
      "63/78 [=======================>......] - ETA: 0s - loss: 0.1400 - accuracy: 0.9476\n",
      "Epoch 10: val_loss did not improve from 0.12682\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.1353 - accuracy: 0.9497 - val_loss: 0.1289 - val_accuracy: 0.9497\n",
      "Epoch 11/200\n",
      "64/78 [=======================>......] - ETA: 0s - loss: 0.1260 - accuracy: 0.9519\n",
      "Epoch 11: val_loss did not improve from 0.12682\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.1298 - accuracy: 0.9515 - val_loss: 0.1499 - val_accuracy: 0.9621\n",
      "Epoch 12/200\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.1198 - accuracy: 0.9570\n",
      "Epoch 12: val_loss improved from 0.12682 to 0.10008, saving model to ./deep_model/model_check\\12-0.100084.hdf5\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.1193 - accuracy: 0.9574 - val_loss: 0.1001 - val_accuracy: 0.9600\n",
      "Epoch 13/200\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.1124 - accuracy: 0.9582\n",
      "Epoch 13: val_loss improved from 0.10008 to 0.09413, saving model to ./deep_model/model_check\\13-0.094129.hdf5\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.1128 - accuracy: 0.9584 - val_loss: 0.0941 - val_accuracy: 0.9641\n",
      "Epoch 14/200\n",
      "62/78 [======================>.......] - ETA: 0s - loss: 0.1045 - accuracy: 0.9613\n",
      "Epoch 14: val_loss did not improve from 0.09413\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.1044 - accuracy: 0.9600 - val_loss: 0.0976 - val_accuracy: 0.9754\n",
      "Epoch 15/200\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.1071 - accuracy: 0.9655\n",
      "Epoch 15: val_loss improved from 0.09413 to 0.07967, saving model to ./deep_model/model_check\\15-0.079674.hdf5\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.1080 - accuracy: 0.9648 - val_loss: 0.0797 - val_accuracy: 0.9744\n",
      "Epoch 16/200\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.0918 - accuracy: 0.9710\n",
      "Epoch 16: val_loss improved from 0.07967 to 0.07491, saving model to ./deep_model/model_check\\16-0.074911.hdf5\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0926 - accuracy: 0.9705 - val_loss: 0.0749 - val_accuracy: 0.9754\n",
      "Epoch 17/200\n",
      "72/78 [==========================>...] - ETA: 0s - loss: 0.0888 - accuracy: 0.9719\n",
      "Epoch 17: val_loss did not improve from 0.07491\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0881 - accuracy: 0.9715 - val_loss: 0.0763 - val_accuracy: 0.9692\n",
      "Epoch 18/200\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.0846 - accuracy: 0.9742\n",
      "Epoch 18: val_loss did not improve from 0.07491\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0836 - accuracy: 0.9749 - val_loss: 0.0750 - val_accuracy: 0.9662\n",
      "Epoch 19/200\n",
      "77/78 [============================>.] - ETA: 0s - loss: 0.0841 - accuracy: 0.9756\n",
      "Epoch 19: val_loss improved from 0.07491 to 0.06596, saving model to ./deep_model/model_check\\19-0.065961.hdf5\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0832 - accuracy: 0.9759 - val_loss: 0.0660 - val_accuracy: 0.9774\n",
      "Epoch 20/200\n",
      "74/78 [===========================>..] - ETA: 0s - loss: 0.0792 - accuracy: 0.9749\n",
      "Epoch 20: val_loss did not improve from 0.06596\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0820 - accuracy: 0.9746 - val_loss: 0.0667 - val_accuracy: 0.9785\n",
      "Epoch 21/200\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0848 - accuracy: 0.9744\n",
      "Epoch 21: val_loss did not improve from 0.06596\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0843 - accuracy: 0.9736 - val_loss: 0.0760 - val_accuracy: 0.9795\n",
      "Epoch 22/200\n",
      "63/78 [=======================>......] - ETA: 0s - loss: 0.0731 - accuracy: 0.9775\n",
      "Epoch 22: val_loss improved from 0.06596 to 0.05884, saving model to ./deep_model/model_check\\22-0.058842.hdf5\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0718 - accuracy: 0.9772 - val_loss: 0.0588 - val_accuracy: 0.9805\n",
      "Epoch 23/200\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0691 - accuracy: 0.9788\n",
      "Epoch 23: val_loss improved from 0.05884 to 0.05756, saving model to ./deep_model/model_check\\23-0.057556.hdf5\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0705 - accuracy: 0.9790 - val_loss: 0.0576 - val_accuracy: 0.9795\n",
      "Epoch 24/200\n",
      "64/78 [=======================>......] - ETA: 0s - loss: 0.0743 - accuracy: 0.9762\n",
      "Epoch 24: val_loss did not improve from 0.05756\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0726 - accuracy: 0.9764 - val_loss: 0.0627 - val_accuracy: 0.9795\n",
      "Epoch 25/200\n",
      "68/78 [=========================>....] - ETA: 0s - loss: 0.0717 - accuracy: 0.9785\n",
      "Epoch 25: val_loss improved from 0.05756 to 0.05370, saving model to ./deep_model/model_check\\25-0.053697.hdf5\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0709 - accuracy: 0.9790 - val_loss: 0.0537 - val_accuracy: 0.9795\n",
      "Epoch 26/200\n",
      "72/78 [==========================>...] - ETA: 0s - loss: 0.0663 - accuracy: 0.9808\n",
      "Epoch 26: val_loss improved from 0.05370 to 0.05333, saving model to ./deep_model/model_check\\26-0.053334.hdf5\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0674 - accuracy: 0.9810 - val_loss: 0.0533 - val_accuracy: 0.9815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/200\n",
      "65/78 [========================>.....] - ETA: 0s - loss: 0.0630 - accuracy: 0.9812\n",
      "Epoch 27: val_loss did not improve from 0.05333\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0644 - accuracy: 0.9813 - val_loss: 0.0572 - val_accuracy: 0.9795\n",
      "Epoch 28/200\n",
      "65/78 [========================>.....] - ETA: 0s - loss: 0.0680 - accuracy: 0.9794\n",
      "Epoch 28: val_loss did not improve from 0.05333\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0673 - accuracy: 0.9792 - val_loss: 0.0714 - val_accuracy: 0.9744\n",
      "Epoch 29/200\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0673 - accuracy: 0.9785\n",
      "Epoch 29: val_loss did not improve from 0.05333\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0677 - accuracy: 0.9787 - val_loss: 0.0617 - val_accuracy: 0.9754\n",
      "Epoch 30/200\n",
      "66/78 [========================>.....] - ETA: 0s - loss: 0.0646 - accuracy: 0.9794\n",
      "Epoch 30: val_loss did not improve from 0.05333\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0622 - accuracy: 0.9800 - val_loss: 0.0574 - val_accuracy: 0.9815\n",
      "Epoch 31/200\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0606 - accuracy: 0.9817\n",
      "Epoch 31: val_loss improved from 0.05333 to 0.05317, saving model to ./deep_model/model_check\\31-0.053174.hdf5\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.0642 - accuracy: 0.9802 - val_loss: 0.0532 - val_accuracy: 0.9815\n",
      "Epoch 32/200\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0639 - accuracy: 0.9803\n",
      "Epoch 32: val_loss did not improve from 0.05317\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0664 - accuracy: 0.9797 - val_loss: 0.0635 - val_accuracy: 0.9795\n",
      "Epoch 33/200\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0631 - accuracy: 0.9803\n",
      "Epoch 33: val_loss did not improve from 0.05317\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0613 - accuracy: 0.9810 - val_loss: 0.0540 - val_accuracy: 0.9805\n",
      "Epoch 34/200\n",
      "69/78 [=========================>....] - ETA: 0s - loss: 0.0715 - accuracy: 0.9754\n",
      "Epoch 34: val_loss did not improve from 0.05317\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0656 - accuracy: 0.9777 - val_loss: 0.0729 - val_accuracy: 0.9754\n",
      "Epoch 35/200\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0525 - accuracy: 0.9833\n",
      "Epoch 35: val_loss did not improve from 0.05317\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.0608 - accuracy: 0.9800 - val_loss: 0.0538 - val_accuracy: 0.9846\n",
      "Epoch 36/200\n",
      "74/78 [===========================>..] - ETA: 0s - loss: 0.0570 - accuracy: 0.9827\n",
      "Epoch 36: val_loss improved from 0.05317 to 0.05237, saving model to ./deep_model/model_check\\36-0.052369.hdf5\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0580 - accuracy: 0.9826 - val_loss: 0.0524 - val_accuracy: 0.9836\n",
      "Epoch 37/200\n",
      "64/78 [=======================>......] - ETA: 0s - loss: 0.0543 - accuracy: 0.9828\n",
      "Epoch 37: val_loss did not improve from 0.05237\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0575 - accuracy: 0.9823 - val_loss: 0.0588 - val_accuracy: 0.9815\n",
      "Epoch 38/200\n",
      "58/78 [=====================>........] - ETA: 0s - loss: 0.0613 - accuracy: 0.9786\n",
      "Epoch 38: val_loss improved from 0.05237 to 0.04930, saving model to ./deep_model/model_check\\38-0.049299.hdf5\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.0597 - accuracy: 0.9802 - val_loss: 0.0493 - val_accuracy: 0.9846\n",
      "Epoch 39/200\n",
      "72/78 [==========================>...] - ETA: 0s - loss: 0.0571 - accuracy: 0.9831\n",
      "Epoch 39: val_loss did not improve from 0.04930\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0577 - accuracy: 0.9831 - val_loss: 0.0549 - val_accuracy: 0.9805\n",
      "Epoch 40/200\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0560 - accuracy: 0.9826\n",
      "Epoch 40: val_loss did not improve from 0.04930\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0559 - accuracy: 0.9818 - val_loss: 0.0547 - val_accuracy: 0.9805\n",
      "Epoch 41/200\n",
      "72/78 [==========================>...] - ETA: 0s - loss: 0.0632 - accuracy: 0.9806\n",
      "Epoch 41: val_loss did not improve from 0.04930\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0630 - accuracy: 0.9805 - val_loss: 0.0538 - val_accuracy: 0.9826\n",
      "Epoch 42/200\n",
      "71/78 [==========================>...] - ETA: 0s - loss: 0.0562 - accuracy: 0.9837\n",
      "Epoch 42: val_loss did not improve from 0.04930\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0534 - accuracy: 0.9846 - val_loss: 0.0562 - val_accuracy: 0.9815\n",
      "Epoch 43/200\n",
      "70/78 [=========================>....] - ETA: 0s - loss: 0.0544 - accuracy: 0.9834\n",
      "Epoch 43: val_loss did not improve from 0.04930\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0533 - accuracy: 0.9841 - val_loss: 0.0510 - val_accuracy: 0.9805\n",
      "Epoch 44/200\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0529 - accuracy: 0.9854\n",
      "Epoch 44: val_loss did not improve from 0.04930\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0547 - accuracy: 0.9851 - val_loss: 0.0549 - val_accuracy: 0.9805\n",
      "Epoch 45/200\n",
      "67/78 [========================>.....] - ETA: 0s - loss: 0.0630 - accuracy: 0.9803\n",
      "Epoch 45: val_loss did not improve from 0.04930\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0596 - accuracy: 0.9818 - val_loss: 0.0519 - val_accuracy: 0.9836\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, validation_split=0.2, batch_size=50, epochs=200, \\\n",
    "                    callbacks=[checkpointer, early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3639bb60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x240ddc56670>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cdf16a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x240ddc94e50>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8b7b853b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 0s 5ms/step - loss: 0.0770 - accuracy: 0.9742\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0769677683711052, 0.9741538166999817]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825f4c05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
